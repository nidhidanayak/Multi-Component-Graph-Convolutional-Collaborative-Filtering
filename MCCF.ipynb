{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MCCF.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPiAb8SFTvgFN12mVI30qiL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nidhidanayak/Multi-Component-Graph-Convolutional-Collaborative-Filtering/blob/master/MCCF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "1UYGVpAutTdX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "009b1050-b961-42b1-91cd-531deb57a139"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Github/\n",
        "git_token= \" \"  #your git token\n",
        "\n",
        "username= \"nidhidanayak\" #your username\n",
        "\n",
        "repository= \"Multi-Component-Graph-Convolutional-Collaborative-Filtering\""
      ],
      "metadata": {
        "id": "brVvqiT0AtC3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e22fcdb-bde2-48f6-ba1e-4f79aeb2cc2b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Github\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://{git_token}@github.com/{username}/{repository}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0WjjZAnIb-r",
        "outputId": "f38af8d3-ecf4-4fbc-9b3d-7af10a216f3f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Multi-Component-Graph-Convolutional-Collaborative-Filtering'...\n",
            "remote: Enumerating objects: 68, done.\u001b[K\n",
            "remote: Counting objects: 100% (68/68), done.\u001b[K\n",
            "remote: Compressing objects: 100% (55/55), done.\u001b[K\n",
            "remote: Total 68 (delta 23), reused 46 (delta 12), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (68/68), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Github/Multi-Component-Graph-Convolutional-Collaborative-Filtering"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NctlVw5NBVzV",
        "outputId": "b1ffe7e6-01c2-40e6-ddee-fd262ed0b87a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Github/Multi-Component-Graph-Convolutional-Collaborative-Filtering\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python run.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyPeRanwBYW4",
        "outputId": "68bb770b-66d5-4f56-9db8-38374b3e0fee"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: yelp\n",
            "-------------------- Hyperparams --------------------\n",
            "N: 30000\n",
            "weight decay: 0.0005\n",
            "dropout rate: 0.5\n",
            "learning rate: 0.001\n",
            "dimension of embedding: 64\n",
            "2022-08-03 15:33:37.330638 Training: [1 epoch,  10 batch] loss: 43.02707, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:33:45.863091 Training: [1 epoch,  20 batch] loss: 42.42303, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:33:54.332961 Training: [1 epoch,  30 batch] loss: 42.00899, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:34:02.858544 Training: [1 epoch,  40 batch] loss: 41.81264, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:34:11.761120 Training: [1 epoch,  50 batch] loss: 41.69831, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:34:20.844937 Training: [1 epoch,  60 batch] loss: 41.61350, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:34:29.423613 Training: [1 epoch,  70 batch] loss: 41.52994, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:34:37.982236 Training: [1 epoch,  80 batch] loss: 41.40965, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:34:46.623097 Training: [1 epoch,  90 batch] loss: 41.35451, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:34:55.324811 Training: [1 epoch, 100 batch] loss: 41.26853, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:35:03.966543 Training: [1 epoch, 110 batch] loss: 41.21390, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:35:12.553463 Training: [1 epoch, 120 batch] loss: 41.13419, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:35:21.248928 Training: [1 epoch, 130 batch] loss: 41.05774, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:35:29.925911 Training: [1 epoch, 140 batch] loss: 41.02324, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:35:38.577055 Training: [1 epoch, 150 batch] loss: 40.83817, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:35:47.276896 Training: [1 epoch, 160 batch] loss: 40.80947, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:35:55.973305 Training: [1 epoch, 170 batch] loss: 40.74362, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:36:04.597466 Training: [1 epoch, 180 batch] loss: 40.59872, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:36:13.262846 Training: [1 epoch, 190 batch] loss: 40.69973, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:36:21.921552 Training: [1 epoch, 200 batch] loss: 40.43122, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:36:30.626084 Training: [1 epoch, 210 batch] loss: 40.35577, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:36:40.166584 Training: [1 epoch, 220 batch] loss: 40.32778, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:36:48.817577 Training: [1 epoch, 230 batch] loss: 40.17775, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:36:57.470890 Training: [1 epoch, 240 batch] loss: 40.09875, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:37:06.114464 Training: [1 epoch, 250 batch] loss: 40.03945, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:37:14.746128 Training: [1 epoch, 260 batch] loss: 39.96331, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:37:23.499003 Training: [1 epoch, 270 batch] loss: 39.89438, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:37:32.155230 Training: [1 epoch, 280 batch] loss: 39.79430, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:37:40.852927 Training: [1 epoch, 290 batch] loss: 39.74479, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:37:49.545429 Training: [1 epoch, 300 batch] loss: 39.62606, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:37:58.196051 Training: [1 epoch, 310 batch] loss: 39.57766, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:38:06.832003 Training: [1 epoch, 320 batch] loss: 39.44452, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:38:15.489554 Training: [1 epoch, 330 batch] loss: 39.38849, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:38:24.115739 Training: [1 epoch, 340 batch] loss: 39.29746, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:38:32.781964 Training: [1 epoch, 350 batch] loss: 39.26631, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:38:41.429377 Training: [1 epoch, 360 batch] loss: 39.27843, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:38:50.126854 Training: [1 epoch, 370 batch] loss: 39.02477, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:38:59.643974 Training: [1 epoch, 380 batch] loss: 38.95921, the best RMSE/MAE: inf / inf\n",
            "<Test> RMSE: 1487.40596, MAE: 1143.92163 \n",
            "2022-08-03 15:39:46.575091 Training: [2 epoch,  10 batch] loss: 38.75373, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:39:55.173250 Training: [2 epoch,  20 batch] loss: 38.80545, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:40:03.763127 Training: [2 epoch,  30 batch] loss: 38.53576, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:40:12.351896 Training: [2 epoch,  40 batch] loss: 38.45276, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:40:21.037544 Training: [2 epoch,  50 batch] loss: 38.37628, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:40:29.669090 Training: [2 epoch,  60 batch] loss: 38.25513, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:40:38.235972 Training: [2 epoch,  70 batch] loss: 38.18948, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:40:46.858445 Training: [2 epoch,  80 batch] loss: 38.15635, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:40:55.450281 Training: [2 epoch,  90 batch] loss: 37.95971, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:41:04.086616 Training: [2 epoch, 100 batch] loss: 37.99641, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:41:12.697450 Training: [2 epoch, 110 batch] loss: 37.73373, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:41:22.202454 Training: [2 epoch, 120 batch] loss: 37.65589, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:41:30.845718 Training: [2 epoch, 130 batch] loss: 37.54644, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:41:39.464037 Training: [2 epoch, 140 batch] loss: 37.48993, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:41:48.065460 Training: [2 epoch, 150 batch] loss: 37.47233, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:41:56.720459 Training: [2 epoch, 160 batch] loss: 37.24749, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:42:05.343301 Training: [2 epoch, 170 batch] loss: 37.12362, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:42:13.968571 Training: [2 epoch, 180 batch] loss: 37.05157, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:42:22.523594 Training: [2 epoch, 190 batch] loss: 36.96315, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:42:31.154841 Training: [2 epoch, 200 batch] loss: 36.80020, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:42:39.830430 Training: [2 epoch, 210 batch] loss: 36.66930, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:42:48.468080 Training: [2 epoch, 220 batch] loss: 36.57349, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:42:57.093751 Training: [2 epoch, 230 batch] loss: 36.49389, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:43:05.664175 Training: [2 epoch, 240 batch] loss: 36.37734, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:43:14.293652 Training: [2 epoch, 250 batch] loss: 36.26405, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:43:22.936577 Training: [2 epoch, 260 batch] loss: 36.16916, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:43:31.559129 Training: [2 epoch, 270 batch] loss: 36.01200, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:43:40.544677 Training: [2 epoch, 280 batch] loss: 35.90856, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:43:49.714216 Training: [2 epoch, 290 batch] loss: 35.77499, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:43:58.358502 Training: [2 epoch, 300 batch] loss: 35.65109, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:44:06.936990 Training: [2 epoch, 310 batch] loss: 35.54812, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:44:15.607505 Training: [2 epoch, 320 batch] loss: 35.43797, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:44:24.276934 Training: [2 epoch, 330 batch] loss: 35.30574, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:44:32.919248 Training: [2 epoch, 340 batch] loss: 35.19722, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:44:41.527390 Training: [2 epoch, 350 batch] loss: 35.06015, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:44:50.193831 Training: [2 epoch, 360 batch] loss: 34.99198, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:44:58.831596 Training: [2 epoch, 370 batch] loss: 34.88928, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:45:07.498825 Training: [2 epoch, 380 batch] loss: 34.76005, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "<Test> RMSE: 7.36587, MAE: 5.62144 \n",
            "2022-08-03 15:45:54.083839 Training: [3 epoch,  10 batch] loss: 34.51642, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:46:02.812600 Training: [3 epoch,  20 batch] loss: 34.38752, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:46:12.040018 Training: [3 epoch,  30 batch] loss: 34.23899, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:46:20.643716 Training: [3 epoch,  40 batch] loss: 34.23529, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:46:29.212817 Training: [3 epoch,  50 batch] loss: 34.03435, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:46:37.743344 Training: [3 epoch,  60 batch] loss: 33.88307, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:46:46.347063 Training: [3 epoch,  70 batch] loss: 33.79030, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:46:54.949886 Training: [3 epoch,  80 batch] loss: 33.63566, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:47:03.619805 Training: [3 epoch,  90 batch] loss: 33.51667, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:47:12.210174 Training: [3 epoch, 100 batch] loss: 33.35572, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:47:20.795371 Training: [3 epoch, 110 batch] loss: 33.27535, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:47:29.465782 Training: [3 epoch, 120 batch] loss: 33.21319, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:47:38.075074 Training: [3 epoch, 130 batch] loss: 33.02592, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:47:46.656043 Training: [3 epoch, 140 batch] loss: 32.83657, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:47:55.258743 Training: [3 epoch, 150 batch] loss: 32.81558, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:48:03.861097 Training: [3 epoch, 160 batch] loss: 32.60987, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:48:12.525321 Training: [3 epoch, 170 batch] loss: 32.44106, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:48:21.158611 Training: [3 epoch, 180 batch] loss: 32.36170, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:48:30.676939 Training: [3 epoch, 190 batch] loss: 32.20784, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:48:39.391249 Training: [3 epoch, 200 batch] loss: 32.04229, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:48:48.016055 Training: [3 epoch, 210 batch] loss: 32.13772, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:48:56.598898 Training: [3 epoch, 220 batch] loss: 31.77292, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:49:05.305785 Training: [3 epoch, 230 batch] loss: 31.67636, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:49:13.999005 Training: [3 epoch, 240 batch] loss: 31.52512, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:49:22.668559 Training: [3 epoch, 250 batch] loss: 31.41011, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:49:31.282200 Training: [3 epoch, 260 batch] loss: 31.22666, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:49:39.926842 Training: [3 epoch, 270 batch] loss: 31.12134, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:49:48.590441 Training: [3 epoch, 280 batch] loss: 31.00319, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:49:57.265839 Training: [3 epoch, 290 batch] loss: 30.85152, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:50:05.861600 Training: [3 epoch, 300 batch] loss: 30.65671, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:50:14.517396 Training: [3 epoch, 310 batch] loss: 30.62559, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:50:23.132027 Training: [3 epoch, 320 batch] loss: 30.43149, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:50:31.755671 Training: [3 epoch, 330 batch] loss: 30.25319, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:50:40.378594 Training: [3 epoch, 340 batch] loss: 30.16077, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:50:49.854814 Training: [3 epoch, 350 batch] loss: 29.97889, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:50:58.544064 Training: [3 epoch, 360 batch] loss: 29.92224, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:51:07.198240 Training: [3 epoch, 370 batch] loss: 29.73247, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:51:15.856993 Training: [3 epoch, 380 batch] loss: 29.61423, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "<Test> RMSE: 1.45382, MAE: 1.17113 \n",
            "2022-08-03 15:52:02.567627 Training: [4 epoch,  10 batch] loss: 29.33785, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:52:11.149640 Training: [4 epoch,  20 batch] loss: 29.24046, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:52:19.814206 Training: [4 epoch,  30 batch] loss: 29.11661, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:52:28.392059 Training: [4 epoch,  40 batch] loss: 29.10939, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:52:36.991916 Training: [4 epoch,  50 batch] loss: 28.83882, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:52:45.611193 Training: [4 epoch,  60 batch] loss: 28.63354, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:52:54.176546 Training: [4 epoch,  70 batch] loss: 28.59102, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:53:02.820638 Training: [4 epoch,  80 batch] loss: 28.44430, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:53:12.368238 Training: [4 epoch,  90 batch] loss: 28.19619, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:53:21.026319 Training: [4 epoch, 100 batch] loss: 28.06441, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:53:29.692265 Training: [4 epoch, 110 batch] loss: 27.92451, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:53:38.407932 Training: [4 epoch, 120 batch] loss: 27.82464, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:53:47.073587 Training: [4 epoch, 130 batch] loss: 27.68499, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:53:55.639542 Training: [4 epoch, 140 batch] loss: 27.48717, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:54:04.259233 Training: [4 epoch, 150 batch] loss: 27.38679, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:54:12.922996 Training: [4 epoch, 160 batch] loss: 27.22608, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:54:21.522316 Training: [4 epoch, 170 batch] loss: 27.09432, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:54:30.201369 Training: [4 epoch, 180 batch] loss: 26.91505, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:54:38.841639 Training: [4 epoch, 190 batch] loss: 26.79175, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:54:47.525286 Training: [4 epoch, 200 batch] loss: 26.67196, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:54:56.154963 Training: [4 epoch, 210 batch] loss: 26.56617, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:55:04.879487 Training: [4 epoch, 220 batch] loss: 26.37556, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:55:13.591426 Training: [4 epoch, 230 batch] loss: 26.30246, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:55:22.274293 Training: [4 epoch, 240 batch] loss: 26.14197, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:55:31.782272 Training: [4 epoch, 250 batch] loss: 25.93453, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:55:40.453696 Training: [4 epoch, 260 batch] loss: 25.87473, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:55:49.116335 Training: [4 epoch, 270 batch] loss: 25.66016, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:55:57.761869 Training: [4 epoch, 280 batch] loss: 25.54184, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:56:06.390871 Training: [4 epoch, 290 batch] loss: 25.41936, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:56:15.100146 Training: [4 epoch, 300 batch] loss: 25.23953, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:56:23.782378 Training: [4 epoch, 310 batch] loss: 25.13667, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:56:32.566899 Training: [4 epoch, 320 batch] loss: 24.95681, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:56:41.278952 Training: [4 epoch, 330 batch] loss: 24.83940, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:56:49.935533 Training: [4 epoch, 340 batch] loss: 24.74283, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:56:58.584910 Training: [4 epoch, 350 batch] loss: 24.53729, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:57:07.231260 Training: [4 epoch, 360 batch] loss: 24.39456, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:57:15.869782 Training: [4 epoch, 370 batch] loss: 24.28398, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:57:24.488775 Training: [4 epoch, 380 batch] loss: 24.10797, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "<Test> RMSE: 0.63661, MAE: 0.52145 \n",
            "2022-08-03 15:58:12.133849 Training: [5 epoch,  10 batch] loss: 23.89868, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 15:58:20.752029 Training: [5 epoch,  20 batch] loss: 23.78967, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 15:58:29.395844 Training: [5 epoch,  30 batch] loss: 23.64103, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 15:58:38.081572 Training: [5 epoch,  40 batch] loss: 23.61768, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 15:58:46.723964 Training: [5 epoch,  50 batch] loss: 23.40340, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 15:58:55.374891 Training: [5 epoch,  60 batch] loss: 23.19980, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 15:59:04.024337 Training: [5 epoch,  70 batch] loss: 23.08261, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 15:59:12.638599 Training: [5 epoch,  80 batch] loss: 23.10185, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 15:59:21.353902 Training: [5 epoch,  90 batch] loss: 22.83019, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 15:59:30.059361 Training: [5 epoch, 100 batch] loss: 22.70097, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 15:59:38.742063 Training: [5 epoch, 110 batch] loss: 22.59804, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 15:59:47.437732 Training: [5 epoch, 120 batch] loss: 22.40703, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 15:59:56.130008 Training: [5 epoch, 130 batch] loss: 22.30043, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 16:00:05.660301 Training: [5 epoch, 140 batch] loss: 22.12811, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 16:00:14.369367 Training: [5 epoch, 150 batch] loss: 22.08461, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 16:00:23.131354 Training: [5 epoch, 160 batch] loss: 21.86165, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 16:00:31.858124 Training: [5 epoch, 170 batch] loss: 21.73659, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 16:00:40.536230 Training: [5 epoch, 180 batch] loss: 21.62951, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 16:00:49.192348 Training: [5 epoch, 190 batch] loss: 21.48760, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 16:00:57.825760 Training: [5 epoch, 200 batch] loss: 21.45299, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 16:01:06.490476 Training: [5 epoch, 210 batch] loss: 21.24309, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 16:01:15.216948 Training: [5 epoch, 220 batch] loss: 21.12570, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 16:01:23.923175 Training: [5 epoch, 230 batch] loss: 20.97744, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 16:01:32.543114 Training: [5 epoch, 240 batch] loss: 20.85322, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 16:01:41.297492 Training: [5 epoch, 250 batch] loss: 20.73832, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 16:01:50.092420 Training: [5 epoch, 260 batch] loss: 20.59106, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 16:01:58.714355 Training: [5 epoch, 270 batch] loss: 20.47870, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 16:02:07.419810 Training: [5 epoch, 280 batch] loss: 20.39852, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 16:02:16.074722 Training: [5 epoch, 290 batch] loss: 20.22985, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 16:02:25.586889 Training: [5 epoch, 300 batch] loss: 20.07580, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 16:02:34.271525 Training: [5 epoch, 310 batch] loss: 19.99372, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 16:02:42.877120 Training: [5 epoch, 320 batch] loss: 19.85118, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 16:02:51.495842 Training: [5 epoch, 330 batch] loss: 19.75574, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 16:03:00.159929 Training: [5 epoch, 340 batch] loss: 19.63319, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 16:03:08.802207 Training: [5 epoch, 350 batch] loss: 19.50928, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 16:03:17.390026 Training: [5 epoch, 360 batch] loss: 19.36549, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 16:03:26.065368 Training: [5 epoch, 370 batch] loss: 19.27883, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 16:03:34.743292 Training: [5 epoch, 380 batch] loss: 19.16750, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "<Test> RMSE: 0.39494, MAE: 0.20803 \n",
            "2022-08-03 16:04:21.541076 Training: [6 epoch,  10 batch] loss: 18.94125, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:04:30.225445 Training: [6 epoch,  20 batch] loss: 18.80496, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:04:39.044492 Training: [6 epoch,  30 batch] loss: 18.66458, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:04:48.274917 Training: [6 epoch,  40 batch] loss: 18.61694, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:04:56.907182 Training: [6 epoch,  50 batch] loss: 18.44386, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:05:05.523075 Training: [6 epoch,  60 batch] loss: 18.34381, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:05:14.130228 Training: [6 epoch,  70 batch] loss: 18.25650, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:05:22.742338 Training: [6 epoch,  80 batch] loss: 18.18441, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:05:31.374065 Training: [6 epoch,  90 batch] loss: 18.00506, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:05:40.061683 Training: [6 epoch, 100 batch] loss: 17.89946, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:05:48.693915 Training: [6 epoch, 110 batch] loss: 17.77034, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:05:57.402607 Training: [6 epoch, 120 batch] loss: 17.72729, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:06:05.990792 Training: [6 epoch, 130 batch] loss: 17.60107, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:06:14.657648 Training: [6 epoch, 140 batch] loss: 17.48579, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:06:23.308686 Training: [6 epoch, 150 batch] loss: 17.30198, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:06:31.934840 Training: [6 epoch, 160 batch] loss: 17.22518, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:06:40.601676 Training: [6 epoch, 170 batch] loss: 17.08254, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:06:49.275786 Training: [6 epoch, 180 batch] loss: 17.00577, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:06:58.865508 Training: [6 epoch, 190 batch] loss: 16.86964, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:07:07.613570 Training: [6 epoch, 200 batch] loss: 16.77212, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:07:16.285698 Training: [6 epoch, 210 batch] loss: 16.89484, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:07:24.993801 Training: [6 epoch, 220 batch] loss: 16.56113, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:07:33.635598 Training: [6 epoch, 230 batch] loss: 16.43722, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:07:42.294432 Training: [6 epoch, 240 batch] loss: 16.37440, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:07:50.970282 Training: [6 epoch, 250 batch] loss: 16.25014, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:07:59.630972 Training: [6 epoch, 260 batch] loss: 16.26053, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:08:08.292491 Training: [6 epoch, 270 batch] loss: 16.06968, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:08:16.989761 Training: [6 epoch, 280 batch] loss: 15.96379, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:08:25.651527 Training: [6 epoch, 290 batch] loss: 15.87061, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:08:34.391819 Training: [6 epoch, 300 batch] loss: 15.84160, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:08:43.018107 Training: [6 epoch, 310 batch] loss: 15.68821, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:08:51.651334 Training: [6 epoch, 320 batch] loss: 15.51338, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:09:00.318227 Training: [6 epoch, 330 batch] loss: 15.45269, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:09:09.034339 Training: [6 epoch, 340 batch] loss: 15.39353, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:09:18.539501 Training: [6 epoch, 350 batch] loss: 15.25938, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:09:27.173179 Training: [6 epoch, 360 batch] loss: 15.19664, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:09:35.848894 Training: [6 epoch, 370 batch] loss: 15.10675, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:09:44.525356 Training: [6 epoch, 380 batch] loss: 14.93576, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "<Test> RMSE: 0.38818, MAE: 0.16549 \n",
            "2022-08-03 16:10:31.117213 Training: [7 epoch,  10 batch] loss: 14.84063, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:10:39.654540 Training: [7 epoch,  20 batch] loss: 14.73328, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:10:48.240816 Training: [7 epoch,  30 batch] loss: 14.66075, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:10:56.803257 Training: [7 epoch,  40 batch] loss: 14.52128, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:11:05.364719 Training: [7 epoch,  50 batch] loss: 14.44150, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:11:13.939536 Training: [7 epoch,  60 batch] loss: 14.35613, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:11:22.549161 Training: [7 epoch,  70 batch] loss: 14.25803, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:11:32.087249 Training: [7 epoch,  80 batch] loss: 14.17967, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:11:40.719089 Training: [7 epoch,  90 batch] loss: 14.10501, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:11:49.338254 Training: [7 epoch, 100 batch] loss: 13.98324, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:11:58.017293 Training: [7 epoch, 110 batch] loss: 13.87889, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:12:06.639502 Training: [7 epoch, 120 batch] loss: 13.82304, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:12:15.334271 Training: [7 epoch, 130 batch] loss: 13.73423, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:12:23.951618 Training: [7 epoch, 140 batch] loss: 13.63566, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:12:32.618293 Training: [7 epoch, 150 batch] loss: 13.57270, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:12:41.277739 Training: [7 epoch, 160 batch] loss: 13.43064, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:12:49.929325 Training: [7 epoch, 170 batch] loss: 13.37200, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:12:58.525340 Training: [7 epoch, 180 batch] loss: 13.31809, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:13:07.176004 Training: [7 epoch, 190 batch] loss: 13.16862, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:13:15.782358 Training: [7 epoch, 200 batch] loss: 13.20549, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:13:24.436860 Training: [7 epoch, 210 batch] loss: 13.07829, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:13:33.027957 Training: [7 epoch, 220 batch] loss: 12.96132, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:13:41.707725 Training: [7 epoch, 230 batch] loss: 12.86229, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:13:51.290372 Training: [7 epoch, 240 batch] loss: 12.77544, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:14:00.018023 Training: [7 epoch, 250 batch] loss: 12.69836, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:14:08.634131 Training: [7 epoch, 260 batch] loss: 12.70714, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:14:17.264148 Training: [7 epoch, 270 batch] loss: 12.57068, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:14:25.889772 Training: [7 epoch, 280 batch] loss: 12.48284, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:14:34.558302 Training: [7 epoch, 290 batch] loss: 12.46500, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:14:43.197766 Training: [7 epoch, 300 batch] loss: 12.30536, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:14:51.876389 Training: [7 epoch, 310 batch] loss: 12.23095, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:15:00.511893 Training: [7 epoch, 320 batch] loss: 12.17689, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:15:09.187015 Training: [7 epoch, 330 batch] loss: 12.08710, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:15:17.817417 Training: [7 epoch, 340 batch] loss: 12.03888, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:15:26.451654 Training: [7 epoch, 350 batch] loss: 11.94948, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:15:35.051854 Training: [7 epoch, 360 batch] loss: 11.96967, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:15:43.729428 Training: [7 epoch, 370 batch] loss: 11.80189, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:15:52.312984 Training: [7 epoch, 380 batch] loss: 11.91290, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "<Test> RMSE: 0.38851, MAE: 0.15471 \n",
            "2022-08-03 16:16:39.765228 Training: [8 epoch,  10 batch] loss: 11.55808, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:16:48.384458 Training: [8 epoch,  20 batch] loss: 11.54023, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:16:56.947320 Training: [8 epoch,  30 batch] loss: 11.43882, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:17:05.540901 Training: [8 epoch,  40 batch] loss: 11.38105, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:17:14.188106 Training: [8 epoch,  50 batch] loss: 11.29477, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:17:22.857830 Training: [8 epoch,  60 batch] loss: 11.26226, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:17:31.473196 Training: [8 epoch,  70 batch] loss: 11.21238, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:17:40.082403 Training: [8 epoch,  80 batch] loss: 11.11056, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:17:48.679254 Training: [8 epoch,  90 batch] loss: 11.09296, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:17:57.327552 Training: [8 epoch, 100 batch] loss: 10.98168, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:18:06.004245 Training: [8 epoch, 110 batch] loss: 10.89658, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:18:14.644409 Training: [8 epoch, 120 batch] loss: 10.84190, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:18:24.116892 Training: [8 epoch, 130 batch] loss: 10.77703, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:18:32.759030 Training: [8 epoch, 140 batch] loss: 10.73726, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:18:41.402651 Training: [8 epoch, 150 batch] loss: 10.64616, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:18:50.044666 Training: [8 epoch, 160 batch] loss: 10.59368, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:18:58.684401 Training: [8 epoch, 170 batch] loss: 10.55371, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:19:07.336003 Training: [8 epoch, 180 batch] loss: 10.39386, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:19:16.095291 Training: [8 epoch, 190 batch] loss: 10.41013, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:19:24.731071 Training: [8 epoch, 200 batch] loss: 10.35986, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:19:33.335229 Training: [8 epoch, 210 batch] loss: 10.20007, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:19:41.938989 Training: [8 epoch, 220 batch] loss: 10.16612, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:19:50.622100 Training: [8 epoch, 230 batch] loss: 10.15740, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:19:59.296428 Training: [8 epoch, 240 batch] loss: 10.05308, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:20:07.953091 Training: [8 epoch, 250 batch] loss: 10.02056, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:20:16.559005 Training: [8 epoch, 260 batch] loss: 9.94691, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:20:25.232403 Training: [8 epoch, 270 batch] loss: 9.89427, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:20:33.923918 Training: [8 epoch, 280 batch] loss: 9.81638, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:20:43.488308 Training: [8 epoch, 290 batch] loss: 9.73423, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:20:52.128326 Training: [8 epoch, 300 batch] loss: 9.67472, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:21:00.755650 Training: [8 epoch, 310 batch] loss: 9.69097, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:21:09.393858 Training: [8 epoch, 320 batch] loss: 9.60534, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:21:18.066428 Training: [8 epoch, 330 batch] loss: 9.67841, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:21:26.711054 Training: [8 epoch, 340 batch] loss: 9.53977, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:21:35.379246 Training: [8 epoch, 350 batch] loss: 9.42746, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:21:44.072831 Training: [8 epoch, 360 batch] loss: 9.37710, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:21:52.766514 Training: [8 epoch, 370 batch] loss: 9.34826, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:22:01.526685 Training: [8 epoch, 380 batch] loss: 9.24387, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "<Test> RMSE: 0.39366, MAE: 0.19986 \n",
            "2022-08-03 16:22:48.514007 Training: [9 epoch,  10 batch] loss: 9.13558, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:22:58.006665 Training: [9 epoch,  20 batch] loss: 9.11297, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:23:06.661440 Training: [9 epoch,  30 batch] loss: 9.13261, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:23:15.278259 Training: [9 epoch,  40 batch] loss: 9.00863, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:23:23.879182 Training: [9 epoch,  50 batch] loss: 8.90407, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:23:32.508354 Training: [9 epoch,  60 batch] loss: 8.84265, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:23:41.193654 Training: [9 epoch,  70 batch] loss: 8.82922, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:23:49.876517 Training: [9 epoch,  80 batch] loss: 8.76393, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:23:58.541284 Training: [9 epoch,  90 batch] loss: 8.71865, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:24:07.247473 Training: [9 epoch, 100 batch] loss: 8.63020, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:24:15.947962 Training: [9 epoch, 110 batch] loss: 8.58587, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:24:24.719621 Training: [9 epoch, 120 batch] loss: 8.63571, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:24:33.399342 Training: [9 epoch, 130 batch] loss: 8.50017, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:24:42.134728 Training: [9 epoch, 140 batch] loss: 8.45415, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:24:50.808183 Training: [9 epoch, 150 batch] loss: 8.38385, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:24:59.514756 Training: [9 epoch, 160 batch] loss: 8.38501, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:25:08.166584 Training: [9 epoch, 170 batch] loss: 8.35104, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:25:17.712668 Training: [9 epoch, 180 batch] loss: 8.25192, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:25:26.391992 Training: [9 epoch, 190 batch] loss: 8.23824, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:25:35.027137 Training: [9 epoch, 200 batch] loss: 8.44705, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:25:43.694831 Training: [9 epoch, 210 batch] loss: 8.15661, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:25:52.334138 Training: [9 epoch, 220 batch] loss: 8.14117, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:26:01.067888 Training: [9 epoch, 230 batch] loss: 8.08342, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:26:09.691341 Training: [9 epoch, 240 batch] loss: 7.95579, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:26:18.351707 Training: [9 epoch, 250 batch] loss: 7.93835, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:26:27.082611 Training: [9 epoch, 260 batch] loss: 7.85835, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:26:35.766558 Training: [9 epoch, 270 batch] loss: 7.86444, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:26:44.418637 Training: [9 epoch, 280 batch] loss: 7.78422, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:26:53.088397 Training: [9 epoch, 290 batch] loss: 7.70759, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:27:01.794726 Training: [9 epoch, 300 batch] loss: 7.69817, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:27:10.481409 Training: [9 epoch, 310 batch] loss: 7.68879, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:27:19.215966 Training: [9 epoch, 320 batch] loss: 7.60760, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:27:27.899773 Training: [9 epoch, 330 batch] loss: 7.66640, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:27:37.523429 Training: [9 epoch, 340 batch] loss: 7.53818, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:27:46.242932 Training: [9 epoch, 350 batch] loss: 7.47298, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:27:54.887433 Training: [9 epoch, 360 batch] loss: 7.44449, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:28:03.541539 Training: [9 epoch, 370 batch] loss: 7.45856, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:28:12.232664 Training: [9 epoch, 380 batch] loss: 7.36874, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "<Test> RMSE: 0.39445, MAE: 0.21610 \n",
            "2022-08-03 16:28:59.121017 Training: [10 epoch,  10 batch] loss: 7.28832, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:29:07.705476 Training: [10 epoch,  20 batch] loss: 7.33772, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:29:16.351399 Training: [10 epoch,  30 batch] loss: 7.20450, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:29:24.946609 Training: [10 epoch,  40 batch] loss: 7.22760, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:29:33.609852 Training: [10 epoch,  50 batch] loss: 7.09339, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:29:42.310585 Training: [10 epoch,  60 batch] loss: 7.05625, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:29:51.784564 Training: [10 epoch,  70 batch] loss: 7.08530, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:30:00.475841 Training: [10 epoch,  80 batch] loss: 7.02785, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:30:09.163674 Training: [10 epoch,  90 batch] loss: 7.03318, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:30:17.800176 Training: [10 epoch, 100 batch] loss: 6.89974, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:30:26.470422 Training: [10 epoch, 110 batch] loss: 7.04166, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:30:35.106514 Training: [10 epoch, 120 batch] loss: 6.84128, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:30:43.814606 Training: [10 epoch, 130 batch] loss: 6.79242, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:30:52.471960 Training: [10 epoch, 140 batch] loss: 6.76178, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:31:01.157902 Training: [10 epoch, 150 batch] loss: 6.72318, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:31:09.765480 Training: [10 epoch, 160 batch] loss: 6.68254, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:31:18.492057 Training: [10 epoch, 170 batch] loss: 6.63675, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:31:27.191737 Training: [10 epoch, 180 batch] loss: 6.63696, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:31:35.883359 Training: [10 epoch, 190 batch] loss: 6.59606, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:31:44.472031 Training: [10 epoch, 200 batch] loss: 6.57932, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:31:53.138910 Training: [10 epoch, 210 batch] loss: 6.48812, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:32:01.836588 Training: [10 epoch, 220 batch] loss: 6.45759, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:32:11.394509 Training: [10 epoch, 230 batch] loss: 6.41307, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:32:20.046139 Training: [10 epoch, 240 batch] loss: 6.40544, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:32:28.759174 Training: [10 epoch, 250 batch] loss: 6.34509, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:32:37.512075 Training: [10 epoch, 260 batch] loss: 6.32817, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:32:46.287798 Training: [10 epoch, 270 batch] loss: 6.25086, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:32:54.986075 Training: [10 epoch, 280 batch] loss: 6.30447, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:33:03.652898 Training: [10 epoch, 290 batch] loss: 6.26822, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:33:12.299271 Training: [10 epoch, 300 batch] loss: 6.19764, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:33:20.941737 Training: [10 epoch, 310 batch] loss: 6.16028, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:33:29.605397 Training: [10 epoch, 320 batch] loss: 6.15999, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:33:38.285814 Training: [10 epoch, 330 batch] loss: 6.10216, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:33:46.937082 Training: [10 epoch, 340 batch] loss: 6.06202, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:33:55.636454 Training: [10 epoch, 350 batch] loss: 6.07884, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:34:04.354246 Training: [10 epoch, 360 batch] loss: 6.01971, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:34:13.020150 Training: [10 epoch, 370 batch] loss: 5.93677, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:34:22.518402 Training: [10 epoch, 380 batch] loss: 5.92853, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "<Test> RMSE: 0.40321, MAE: 0.24876 \n",
            "2022-08-03 16:35:09.140991 Training: [11 epoch,  10 batch] loss: 5.86511, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:35:17.740944 Training: [11 epoch,  20 batch] loss: 5.88853, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:35:26.362156 Training: [11 epoch,  30 batch] loss: 5.77244, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:35:35.002397 Training: [11 epoch,  40 batch] loss: 5.78862, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:35:43.598461 Training: [11 epoch,  50 batch] loss: 5.71747, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:35:52.148994 Training: [11 epoch,  60 batch] loss: 5.72868, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:36:00.738215 Training: [11 epoch,  70 batch] loss: 5.66120, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:36:09.336446 Training: [11 epoch,  80 batch] loss: 5.63958, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:36:17.960917 Training: [11 epoch,  90 batch] loss: 5.77577, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:36:26.620129 Training: [11 epoch, 100 batch] loss: 5.63078, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:36:35.789182 Training: [11 epoch, 110 batch] loss: 5.54249, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:36:44.771719 Training: [11 epoch, 120 batch] loss: 5.58782, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:36:53.506554 Training: [11 epoch, 130 batch] loss: 5.52214, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:37:02.236626 Training: [11 epoch, 140 batch] loss: 5.50745, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:37:10.953790 Training: [11 epoch, 150 batch] loss: 5.52668, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:37:19.610994 Training: [11 epoch, 160 batch] loss: 5.39417, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:37:28.296244 Training: [11 epoch, 170 batch] loss: 5.37706, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:37:37.012084 Training: [11 epoch, 180 batch] loss: 5.36461, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:37:45.653417 Training: [11 epoch, 190 batch] loss: 5.40190, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:37:54.394265 Training: [11 epoch, 200 batch] loss: 5.27604, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:38:03.154295 Training: [11 epoch, 210 batch] loss: 5.27491, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:38:11.898680 Training: [11 epoch, 220 batch] loss: 5.22390, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:38:20.525872 Training: [11 epoch, 230 batch] loss: 5.26812, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:38:29.270687 Training: [11 epoch, 240 batch] loss: 5.15845, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:38:37.967246 Training: [11 epoch, 250 batch] loss: 5.18009, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:38:46.602222 Training: [11 epoch, 260 batch] loss: 5.17297, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:38:56.080059 Training: [11 epoch, 270 batch] loss: 5.10567, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:39:04.764529 Training: [11 epoch, 280 batch] loss: 5.06438, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:39:13.461732 Training: [11 epoch, 290 batch] loss: 5.04705, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:39:22.151340 Training: [11 epoch, 300 batch] loss: 5.00399, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:39:30.823789 Training: [11 epoch, 310 batch] loss: 4.99061, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:39:39.512934 Training: [11 epoch, 320 batch] loss: 4.99835, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:39:48.143233 Training: [11 epoch, 330 batch] loss: 4.92015, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:39:56.854548 Training: [11 epoch, 340 batch] loss: 4.89887, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:40:05.457431 Training: [11 epoch, 350 batch] loss: 4.89928, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:40:14.122135 Training: [11 epoch, 360 batch] loss: 4.82947, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:40:22.789837 Training: [11 epoch, 370 batch] loss: 4.81724, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:40:31.474061 Training: [11 epoch, 380 batch] loss: 4.78070, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "<Test> RMSE: 0.40062, MAE: 0.24000 \n",
            "2022-08-03 16:41:19.316439 Training: [12 epoch,  10 batch] loss: 4.76884, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:41:28.160097 Training: [12 epoch,  20 batch] loss: 4.77053, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:41:36.965397 Training: [12 epoch,  30 batch] loss: 4.77741, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:41:45.817701 Training: [12 epoch,  40 batch] loss: 4.67486, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:41:54.412931 Training: [12 epoch,  50 batch] loss: 4.63793, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:42:03.019575 Training: [12 epoch,  60 batch] loss: 4.70033, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:42:11.657808 Training: [12 epoch,  70 batch] loss: 4.67445, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:42:20.305516 Training: [12 epoch,  80 batch] loss: 4.58718, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:42:28.981555 Training: [12 epoch,  90 batch] loss: 4.57378, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:42:37.627140 Training: [12 epoch, 100 batch] loss: 4.54099, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:42:46.258744 Training: [12 epoch, 110 batch] loss: 4.50735, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:42:54.865529 Training: [12 epoch, 120 batch] loss: 4.67511, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:43:03.593937 Training: [12 epoch, 130 batch] loss: 4.47547, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:43:12.286049 Training: [12 epoch, 140 batch] loss: 4.45773, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:43:20.965010 Training: [12 epoch, 150 batch] loss: 4.50839, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:43:30.493920 Training: [12 epoch, 160 batch] loss: 4.39094, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:43:39.159814 Training: [12 epoch, 170 batch] loss: 4.36020, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:43:47.824325 Training: [12 epoch, 180 batch] loss: 4.33687, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:43:56.424898 Training: [12 epoch, 190 batch] loss: 4.32924, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:44:05.113614 Training: [12 epoch, 200 batch] loss: 4.33838, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:44:13.780757 Training: [12 epoch, 210 batch] loss: 4.28617, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:44:22.428888 Training: [12 epoch, 220 batch] loss: 4.24777, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:44:31.155528 Training: [12 epoch, 230 batch] loss: 4.35326, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:44:39.858410 Training: [12 epoch, 240 batch] loss: 4.29595, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:44:48.601876 Training: [12 epoch, 250 batch] loss: 4.20610, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:44:57.224960 Training: [12 epoch, 260 batch] loss: 4.18741, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:45:05.920064 Training: [12 epoch, 270 batch] loss: 4.19277, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:45:14.652396 Training: [12 epoch, 280 batch] loss: 4.13967, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:45:23.378596 Training: [12 epoch, 290 batch] loss: 4.17846, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:45:32.021950 Training: [12 epoch, 300 batch] loss: 4.14274, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:45:41.552822 Training: [12 epoch, 310 batch] loss: 4.10456, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:45:50.187249 Training: [12 epoch, 320 batch] loss: 4.11844, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:45:58.901071 Training: [12 epoch, 330 batch] loss: 4.06272, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:46:07.543097 Training: [12 epoch, 340 batch] loss: 3.98053, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:46:16.276834 Training: [12 epoch, 350 batch] loss: 3.98323, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:46:24.958541 Training: [12 epoch, 360 batch] loss: 3.95467, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:46:33.655643 Training: [12 epoch, 370 batch] loss: 3.97910, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:46:42.288036 Training: [12 epoch, 380 batch] loss: 3.93963, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "<Test> RMSE: 0.44829, MAE: 0.34836 \n",
            "2022-08-03 16:47:28.763159 Training: [13 epoch,  10 batch] loss: 3.89243, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:47:37.413312 Training: [13 epoch,  20 batch] loss: 3.92400, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:47:46.037610 Training: [13 epoch,  30 batch] loss: 4.04085, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:47:55.331210 Training: [13 epoch,  40 batch] loss: 3.93034, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:48:04.035738 Training: [13 epoch,  50 batch] loss: 3.81463, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:48:12.674376 Training: [13 epoch,  60 batch] loss: 3.81127, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:48:21.346628 Training: [13 epoch,  70 batch] loss: 3.78197, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:48:29.988625 Training: [13 epoch,  80 batch] loss: 3.84528, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:48:38.597909 Training: [13 epoch,  90 batch] loss: 3.71777, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:48:47.287273 Training: [13 epoch, 100 batch] loss: 3.72106, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:48:55.976076 Training: [13 epoch, 110 batch] loss: 3.72744, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:49:04.679130 Training: [13 epoch, 120 batch] loss: 3.69724, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:49:13.359388 Training: [13 epoch, 130 batch] loss: 3.67265, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:49:22.009827 Training: [13 epoch, 140 batch] loss: 3.65130, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:49:30.710580 Training: [13 epoch, 150 batch] loss: 3.71165, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:49:39.376282 Training: [13 epoch, 160 batch] loss: 3.62823, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:49:48.044994 Training: [13 epoch, 170 batch] loss: 3.60004, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:49:56.726660 Training: [13 epoch, 180 batch] loss: 3.58782, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:50:05.451951 Training: [13 epoch, 190 batch] loss: 3.60219, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:50:15.060607 Training: [13 epoch, 200 batch] loss: 3.54119, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:50:23.700040 Training: [13 epoch, 210 batch] loss: 3.58997, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:50:32.461809 Training: [13 epoch, 220 batch] loss: 3.49615, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:50:41.137230 Training: [13 epoch, 230 batch] loss: 3.46637, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:50:49.794780 Training: [13 epoch, 240 batch] loss: 3.49551, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:50:58.425605 Training: [13 epoch, 250 batch] loss: 3.45339, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:51:07.126744 Training: [13 epoch, 260 batch] loss: 3.45242, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:51:15.796573 Training: [13 epoch, 270 batch] loss: 3.40716, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:51:24.451983 Training: [13 epoch, 280 batch] loss: 3.39836, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:51:33.154506 Training: [13 epoch, 290 batch] loss: 3.46926, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:51:41.885493 Training: [13 epoch, 300 batch] loss: 3.39544, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:51:50.543991 Training: [13 epoch, 310 batch] loss: 3.40715, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:51:59.199000 Training: [13 epoch, 320 batch] loss: 3.43750, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:52:07.849394 Training: [13 epoch, 330 batch] loss: 3.39513, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:52:16.537728 Training: [13 epoch, 340 batch] loss: 3.31968, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:52:25.757403 Training: [13 epoch, 350 batch] loss: 3.36403, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:52:34.803126 Training: [13 epoch, 360 batch] loss: 3.26690, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:52:43.486950 Training: [13 epoch, 370 batch] loss: 3.28172, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:52:52.182207 Training: [13 epoch, 380 batch] loss: 3.32229, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "<Test> RMSE: 0.40823, MAE: 0.26358 \n",
            "2022-08-03 16:53:38.828429 Training: [14 epoch,  10 batch] loss: 3.24210, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:53:47.437894 Training: [14 epoch,  20 batch] loss: 3.27776, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:53:56.049097 Training: [14 epoch,  30 batch] loss: 3.21750, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:54:04.659723 Training: [14 epoch,  40 batch] loss: 3.22322, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:54:13.363453 Training: [14 epoch,  50 batch] loss: 3.17403, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:54:22.009281 Training: [14 epoch,  60 batch] loss: 3.14373, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:54:30.688052 Training: [14 epoch,  70 batch] loss: 3.12202, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:54:39.322048 Training: [14 epoch,  80 batch] loss: 3.13182, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:54:48.847231 Training: [14 epoch,  90 batch] loss: 3.13372, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:54:57.494332 Training: [14 epoch, 100 batch] loss: 3.07674, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:55:06.177834 Training: [14 epoch, 110 batch] loss: 3.07386, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:55:14.804319 Training: [14 epoch, 120 batch] loss: 3.08354, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:55:23.468287 Training: [14 epoch, 130 batch] loss: 3.05027, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:55:32.201327 Training: [14 epoch, 140 batch] loss: 3.03819, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:55:40.892683 Training: [14 epoch, 150 batch] loss: 3.01526, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:55:49.572065 Training: [14 epoch, 160 batch] loss: 3.15213, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:55:58.225922 Training: [14 epoch, 170 batch] loss: 3.04190, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:56:06.887714 Training: [14 epoch, 180 batch] loss: 3.02141, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:56:15.593665 Training: [14 epoch, 190 batch] loss: 2.94665, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:56:24.285174 Training: [14 epoch, 200 batch] loss: 2.93305, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:56:33.037304 Training: [14 epoch, 210 batch] loss: 2.98988, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:56:41.728855 Training: [14 epoch, 220 batch] loss: 2.91491, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:56:50.374461 Training: [14 epoch, 230 batch] loss: 2.92382, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:56:59.914982 Training: [14 epoch, 240 batch] loss: 2.94191, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:57:08.616783 Training: [14 epoch, 250 batch] loss: 2.87844, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:57:17.317462 Training: [14 epoch, 260 batch] loss: 2.88082, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:57:26.039012 Training: [14 epoch, 270 batch] loss: 2.85378, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:57:34.705230 Training: [14 epoch, 280 batch] loss: 2.82850, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:57:43.394314 Training: [14 epoch, 290 batch] loss: 2.89645, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:57:52.085707 Training: [14 epoch, 300 batch] loss: 2.81591, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:58:00.737388 Training: [14 epoch, 310 batch] loss: 2.84271, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:58:09.453664 Training: [14 epoch, 320 batch] loss: 2.81859, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:58:18.086386 Training: [14 epoch, 330 batch] loss: 2.86383, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:58:26.716724 Training: [14 epoch, 340 batch] loss: 2.72703, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:58:35.462488 Training: [14 epoch, 350 batch] loss: 2.75933, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:58:44.187395 Training: [14 epoch, 360 batch] loss: 2.72083, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:58:52.810747 Training: [14 epoch, 370 batch] loss: 2.71724, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:59:01.545327 Training: [14 epoch, 380 batch] loss: 2.69705, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "<Test> RMSE: 0.41102, MAE: 0.27116 \n",
            "2022-08-03 16:59:49.227914 Training: [15 epoch,  10 batch] loss: 2.67779, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:59:57.834853 Training: [15 epoch,  20 batch] loss: 2.67556, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:00:06.362818 Training: [15 epoch,  30 batch] loss: 2.64094, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:00:15.021108 Training: [15 epoch,  40 batch] loss: 2.69877, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:00:23.681671 Training: [15 epoch,  50 batch] loss: 2.63020, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:00:32.391463 Training: [15 epoch,  60 batch] loss: 2.64213, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:00:41.067188 Training: [15 epoch,  70 batch] loss: 2.60040, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:00:49.750750 Training: [15 epoch,  80 batch] loss: 2.60480, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:00:58.430095 Training: [15 epoch,  90 batch] loss: 2.57909, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:01:07.106028 Training: [15 epoch, 100 batch] loss: 2.57264, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:01:15.759600 Training: [15 epoch, 110 batch] loss: 2.59241, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:01:24.384215 Training: [15 epoch, 120 batch] loss: 2.62750, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:01:33.901445 Training: [15 epoch, 130 batch] loss: 2.51970, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:01:42.607462 Training: [15 epoch, 140 batch] loss: 2.52274, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:01:51.234870 Training: [15 epoch, 150 batch] loss: 2.50095, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:01:59.890453 Training: [15 epoch, 160 batch] loss: 2.50276, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:02:08.585503 Training: [15 epoch, 170 batch] loss: 2.50726, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:02:17.330273 Training: [15 epoch, 180 batch] loss: 2.51407, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:02:26.040934 Training: [15 epoch, 190 batch] loss: 2.56885, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:02:34.695014 Training: [15 epoch, 200 batch] loss: 2.47297, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:02:43.367670 Training: [15 epoch, 210 batch] loss: 2.71953, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:02:52.034329 Training: [15 epoch, 220 batch] loss: 2.47265, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:03:00.697127 Training: [15 epoch, 230 batch] loss: 2.48272, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:03:09.325881 Training: [15 epoch, 240 batch] loss: 2.48681, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:03:17.992596 Training: [15 epoch, 250 batch] loss: 2.56575, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:03:26.625533 Training: [15 epoch, 260 batch] loss: 2.48687, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:03:35.336284 Training: [15 epoch, 270 batch] loss: 2.54023, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:03:44.870043 Training: [15 epoch, 280 batch] loss: 2.51812, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:03:53.590715 Training: [15 epoch, 290 batch] loss: 2.49368, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:04:02.240859 Training: [15 epoch, 300 batch] loss: 2.47973, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:04:10.909408 Training: [15 epoch, 310 batch] loss: 2.48993, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:04:19.543385 Training: [15 epoch, 320 batch] loss: 2.47247, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:04:28.253758 Training: [15 epoch, 330 batch] loss: 2.48390, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:04:36.885736 Training: [15 epoch, 340 batch] loss: 2.45610, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:04:45.550188 Training: [15 epoch, 350 batch] loss: 2.46997, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:04:54.205186 Training: [15 epoch, 360 batch] loss: 2.53764, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:05:02.913668 Training: [15 epoch, 370 batch] loss: 2.58214, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:05:11.605805 Training: [15 epoch, 380 batch] loss: 2.48610, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "<Test> RMSE: 0.40132, MAE: 0.24248 \n",
            "2022-08-03 17:05:58.731653 Training: [16 epoch,  10 batch] loss: 2.54444, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:06:07.535569 Training: [16 epoch,  20 batch] loss: 2.44844, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:06:16.104540 Training: [16 epoch,  30 batch] loss: 2.45579, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:06:24.723605 Training: [16 epoch,  40 batch] loss: 2.45556, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:06:33.300198 Training: [16 epoch,  50 batch] loss: 2.47975, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:06:41.895713 Training: [16 epoch,  60 batch] loss: 2.45144, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:06:50.478583 Training: [16 epoch,  70 batch] loss: 2.49764, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:06:59.104757 Training: [16 epoch,  80 batch] loss: 2.45213, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:07:07.723023 Training: [16 epoch,  90 batch] loss: 2.46030, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:07:16.323164 Training: [16 epoch, 100 batch] loss: 2.62926, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:07:24.949776 Training: [16 epoch, 110 batch] loss: 2.48430, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:07:33.576469 Training: [16 epoch, 120 batch] loss: 2.43954, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:07:42.253176 Training: [16 epoch, 130 batch] loss: 2.51214, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:07:50.899664 Training: [16 epoch, 140 batch] loss: 2.46121, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:07:59.506947 Training: [16 epoch, 150 batch] loss: 2.47190, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:08:08.108522 Training: [16 epoch, 160 batch] loss: 2.47603, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:08:17.621172 Training: [16 epoch, 170 batch] loss: 2.45042, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:08:26.315350 Training: [16 epoch, 180 batch] loss: 2.44764, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:08:34.961547 Training: [16 epoch, 190 batch] loss: 2.49287, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:08:43.673113 Training: [16 epoch, 200 batch] loss: 2.45057, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:08:52.401815 Training: [16 epoch, 210 batch] loss: 2.51627, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:09:01.160858 Training: [16 epoch, 220 batch] loss: 2.47774, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:09:09.880831 Training: [16 epoch, 230 batch] loss: 2.47234, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:09:18.521747 Training: [16 epoch, 240 batch] loss: 2.47256, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:09:27.194124 Training: [16 epoch, 250 batch] loss: 2.51151, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:09:35.858685 Training: [16 epoch, 260 batch] loss: 2.44903, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:09:44.499560 Training: [16 epoch, 270 batch] loss: 2.45180, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:09:53.191124 Training: [16 epoch, 280 batch] loss: 2.44765, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:10:01.855862 Training: [16 epoch, 290 batch] loss: 2.45668, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:10:10.588279 Training: [16 epoch, 300 batch] loss: 2.47230, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:10:19.252955 Training: [16 epoch, 310 batch] loss: 2.44591, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:10:28.803817 Training: [16 epoch, 320 batch] loss: 2.44075, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:10:37.490197 Training: [16 epoch, 330 batch] loss: 2.52329, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:10:46.181657 Training: [16 epoch, 340 batch] loss: 2.51249, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:10:54.842287 Training: [16 epoch, 350 batch] loss: 2.47901, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:11:03.493237 Training: [16 epoch, 360 batch] loss: 2.48341, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:11:12.175305 Training: [16 epoch, 370 batch] loss: 2.43977, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:11:20.863972 Training: [16 epoch, 380 batch] loss: 2.49813, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "<Test> RMSE: 0.43033, MAE: 0.31504 \n",
            "2022-08-03 17:12:07.829433 Training: [17 epoch,  10 batch] loss: 2.47452, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:12:16.387553 Training: [17 epoch,  20 batch] loss: 2.47030, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:12:24.987464 Training: [17 epoch,  30 batch] loss: 2.43967, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:12:33.631424 Training: [17 epoch,  40 batch] loss: 2.44104, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:12:43.086807 Training: [17 epoch,  50 batch] loss: 2.44855, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:12:51.668142 Training: [17 epoch,  60 batch] loss: 2.49714, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:13:00.336729 Training: [17 epoch,  70 batch] loss: 2.47586, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:13:08.929544 Training: [17 epoch,  80 batch] loss: 2.44002, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:13:17.553070 Training: [17 epoch,  90 batch] loss: 2.49060, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:13:26.136760 Training: [17 epoch, 100 batch] loss: 2.46079, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:13:34.783027 Training: [17 epoch, 110 batch] loss: 2.54139, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:13:43.408470 Training: [17 epoch, 120 batch] loss: 2.46653, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:13:52.061299 Training: [17 epoch, 130 batch] loss: 2.46200, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:14:00.700279 Training: [17 epoch, 140 batch] loss: 2.52527, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:14:09.449127 Training: [17 epoch, 150 batch] loss: 2.46517, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:14:18.154746 Training: [17 epoch, 160 batch] loss: 2.45254, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:14:26.920221 Training: [17 epoch, 170 batch] loss: 2.44457, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:14:35.590637 Training: [17 epoch, 180 batch] loss: 2.47487, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:14:44.245826 Training: [17 epoch, 190 batch] loss: 2.44183, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:14:53.176758 Training: [17 epoch, 200 batch] loss: 2.44613, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:15:02.422871 Training: [17 epoch, 210 batch] loss: 2.44149, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:15:11.057856 Training: [17 epoch, 220 batch] loss: 2.42502, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:15:19.716312 Training: [17 epoch, 230 batch] loss: 2.49113, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:15:28.375863 Training: [17 epoch, 240 batch] loss: 2.46078, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:15:37.052698 Training: [17 epoch, 250 batch] loss: 2.45013, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:15:45.756140 Training: [17 epoch, 260 batch] loss: 2.45483, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:15:54.445422 Training: [17 epoch, 270 batch] loss: 2.44884, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:16:03.102058 Training: [17 epoch, 280 batch] loss: 2.48362, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:16:11.789642 Training: [17 epoch, 290 batch] loss: 2.44852, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:16:20.412359 Training: [17 epoch, 300 batch] loss: 2.46110, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:16:29.095317 Training: [17 epoch, 310 batch] loss: 2.47000, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:16:37.755908 Training: [17 epoch, 320 batch] loss: 2.49114, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:16:46.412225 Training: [17 epoch, 330 batch] loss: 2.45659, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:16:55.030779 Training: [17 epoch, 340 batch] loss: 2.44374, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:17:03.732209 Training: [17 epoch, 350 batch] loss: 2.42824, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:17:13.328752 Training: [17 epoch, 360 batch] loss: 2.49413, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:17:22.005977 Training: [17 epoch, 370 batch] loss: 2.48691, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:17:30.666214 Training: [17 epoch, 380 batch] loss: 2.47353, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "<Test> RMSE: 0.41943, MAE: 0.29177 \n",
            "2022-08-03 17:18:17.379561 Training: [18 epoch,  10 batch] loss: 2.50396, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:18:25.988838 Training: [18 epoch,  20 batch] loss: 2.54084, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:18:34.628627 Training: [18 epoch,  30 batch] loss: 2.50374, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:18:43.255692 Training: [18 epoch,  40 batch] loss: 2.44105, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:18:51.901818 Training: [18 epoch,  50 batch] loss: 2.48560, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:19:00.580782 Training: [18 epoch,  60 batch] loss: 2.48152, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:19:09.217745 Training: [18 epoch,  70 batch] loss: 2.45799, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:19:17.878019 Training: [18 epoch,  80 batch] loss: 2.43352, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:19:27.423937 Training: [18 epoch,  90 batch] loss: 2.46961, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:19:36.100275 Training: [18 epoch, 100 batch] loss: 2.44045, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:19:44.808870 Training: [18 epoch, 110 batch] loss: 2.49615, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:19:53.518334 Training: [18 epoch, 120 batch] loss: 2.45047, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:20:02.147202 Training: [18 epoch, 130 batch] loss: 2.40772, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:20:10.806989 Training: [18 epoch, 140 batch] loss: 2.43797, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:20:19.467146 Training: [18 epoch, 150 batch] loss: 2.46849, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:20:28.128016 Training: [18 epoch, 160 batch] loss: 2.47229, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:20:36.756124 Training: [18 epoch, 170 batch] loss: 2.44034, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:20:45.480718 Training: [18 epoch, 180 batch] loss: 2.47342, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:20:54.160734 Training: [18 epoch, 190 batch] loss: 2.43797, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:21:02.807363 Training: [18 epoch, 200 batch] loss: 2.46707, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:21:11.482104 Training: [18 epoch, 210 batch] loss: 2.45044, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:21:20.134629 Training: [18 epoch, 220 batch] loss: 2.58704, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:21:28.820308 Training: [18 epoch, 230 batch] loss: 2.48401, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:21:38.400750 Training: [18 epoch, 240 batch] loss: 2.44782, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:21:47.045337 Training: [18 epoch, 250 batch] loss: 2.47770, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:21:55.738558 Training: [18 epoch, 260 batch] loss: 2.46084, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:22:04.412115 Training: [18 epoch, 270 batch] loss: 2.42956, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:22:13.113171 Training: [18 epoch, 280 batch] loss: 2.43813, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:22:21.778546 Training: [18 epoch, 290 batch] loss: 2.42311, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:22:30.506727 Training: [18 epoch, 300 batch] loss: 2.47408, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:22:39.254374 Training: [18 epoch, 310 batch] loss: 2.64358, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:22:47.915309 Training: [18 epoch, 320 batch] loss: 2.46572, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:22:56.569584 Training: [18 epoch, 330 batch] loss: 2.49571, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:23:05.249509 Training: [18 epoch, 340 batch] loss: 2.47092, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:23:13.900451 Training: [18 epoch, 350 batch] loss: 2.44209, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:23:22.582705 Training: [18 epoch, 360 batch] loss: 2.44120, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:23:31.302754 Training: [18 epoch, 370 batch] loss: 2.43103, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:23:39.987943 Training: [18 epoch, 380 batch] loss: 2.45911, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "<Test> RMSE: 0.43372, MAE: 0.32173 \n",
            "2022-08-03 17:24:27.577190 Training: [19 epoch,  10 batch] loss: 2.42348, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:24:36.255825 Training: [19 epoch,  20 batch] loss: 2.56565, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:24:44.860052 Training: [19 epoch,  30 batch] loss: 2.45342, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:24:53.472198 Training: [19 epoch,  40 batch] loss: 2.45446, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:25:02.142955 Training: [19 epoch,  50 batch] loss: 2.44216, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:25:10.755581 Training: [19 epoch,  60 batch] loss: 2.43223, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:25:19.442971 Training: [19 epoch,  70 batch] loss: 2.46424, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:25:28.097014 Training: [19 epoch,  80 batch] loss: 2.48765, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:25:36.776838 Training: [19 epoch,  90 batch] loss: 2.42309, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:25:45.458747 Training: [19 epoch, 100 batch] loss: 2.44755, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:25:54.123141 Training: [19 epoch, 110 batch] loss: 2.47056, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:26:03.657519 Training: [19 epoch, 120 batch] loss: 2.49519, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:26:12.310864 Training: [19 epoch, 130 batch] loss: 2.45054, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:26:21.041238 Training: [19 epoch, 140 batch] loss: 2.46802, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:26:29.764752 Training: [19 epoch, 150 batch] loss: 2.47053, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:26:38.404057 Training: [19 epoch, 160 batch] loss: 2.45361, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:26:47.106082 Training: [19 epoch, 170 batch] loss: 2.46015, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:26:55.804048 Training: [19 epoch, 180 batch] loss: 2.44135, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:27:04.504805 Training: [19 epoch, 190 batch] loss: 2.55323, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:27:13.185233 Training: [19 epoch, 200 batch] loss: 2.41625, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:27:21.890513 Training: [19 epoch, 210 batch] loss: 2.59544, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:27:30.636616 Training: [19 epoch, 220 batch] loss: 2.45600, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:27:39.366393 Training: [19 epoch, 230 batch] loss: 2.49298, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:27:48.049749 Training: [19 epoch, 240 batch] loss: 2.43081, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:27:56.749658 Training: [19 epoch, 250 batch] loss: 2.52482, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:28:05.409908 Training: [19 epoch, 260 batch] loss: 2.46668, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:28:14.920474 Training: [19 epoch, 270 batch] loss: 2.44635, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:28:23.575709 Training: [19 epoch, 280 batch] loss: 2.46724, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:28:32.269920 Training: [19 epoch, 290 batch] loss: 2.46486, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:28:40.959395 Training: [19 epoch, 300 batch] loss: 2.47444, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:28:49.700727 Training: [19 epoch, 310 batch] loss: 2.41417, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:28:58.361649 Training: [19 epoch, 320 batch] loss: 2.46757, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:29:07.075867 Training: [19 epoch, 330 batch] loss: 2.43396, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:29:15.803555 Training: [19 epoch, 340 batch] loss: 2.46506, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:29:24.497123 Training: [19 epoch, 350 batch] loss: 2.43639, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:29:33.192715 Training: [19 epoch, 360 batch] loss: 2.48966, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:29:41.979646 Training: [19 epoch, 370 batch] loss: 2.49382, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:29:50.749334 Training: [19 epoch, 380 batch] loss: 2.43369, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "<Test> RMSE: 0.42449, MAE: 0.30294 \n",
            "2022-08-03 17:30:38.416370 Training: [20 epoch,  10 batch] loss: 2.44888, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:30:47.070885 Training: [20 epoch,  20 batch] loss: 2.44241, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:30:55.679918 Training: [20 epoch,  30 batch] loss: 2.45797, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:31:04.329814 Training: [20 epoch,  40 batch] loss: 2.42491, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:31:12.920070 Training: [20 epoch,  50 batch] loss: 2.44454, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:31:21.511617 Training: [20 epoch,  60 batch] loss: 2.44858, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:31:30.102324 Training: [20 epoch,  70 batch] loss: 2.50455, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:31:38.748688 Training: [20 epoch,  80 batch] loss: 2.41868, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:31:47.481421 Training: [20 epoch,  90 batch] loss: 2.46203, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:31:56.198086 Training: [20 epoch, 100 batch] loss: 2.43308, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:32:04.824824 Training: [20 epoch, 110 batch] loss: 2.43697, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:32:13.484214 Training: [20 epoch, 120 batch] loss: 2.43543, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:32:22.150309 Training: [20 epoch, 130 batch] loss: 2.43733, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:32:31.610175 Training: [20 epoch, 140 batch] loss: 2.47658, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:32:40.395094 Training: [20 epoch, 150 batch] loss: 2.52623, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:32:49.110955 Training: [20 epoch, 160 batch] loss: 2.45192, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:32:57.802020 Training: [20 epoch, 170 batch] loss: 2.43244, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:33:06.506322 Training: [20 epoch, 180 batch] loss: 2.43663, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:33:15.191177 Training: [20 epoch, 190 batch] loss: 2.45508, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:33:23.905130 Training: [20 epoch, 200 batch] loss: 2.66181, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:33:32.588018 Training: [20 epoch, 210 batch] loss: 2.44205, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:33:41.313163 Training: [20 epoch, 220 batch] loss: 2.51286, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:33:49.995028 Training: [20 epoch, 230 batch] loss: 2.51956, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:33:58.701597 Training: [20 epoch, 240 batch] loss: 2.47191, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:34:07.365885 Training: [20 epoch, 250 batch] loss: 2.44594, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:34:16.032499 Training: [20 epoch, 260 batch] loss: 2.46432, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:34:24.717395 Training: [20 epoch, 270 batch] loss: 2.46863, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:34:33.433347 Training: [20 epoch, 280 batch] loss: 2.41871, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:34:42.998472 Training: [20 epoch, 290 batch] loss: 2.45516, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:34:51.747368 Training: [20 epoch, 300 batch] loss: 2.42508, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:35:00.458552 Training: [20 epoch, 310 batch] loss: 2.60555, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:35:09.107932 Training: [20 epoch, 320 batch] loss: 2.47694, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:35:17.795983 Training: [20 epoch, 330 batch] loss: 2.47742, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:35:26.525951 Training: [20 epoch, 340 batch] loss: 2.46491, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:35:35.092749 Training: [20 epoch, 350 batch] loss: 2.43977, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:35:43.790851 Training: [20 epoch, 360 batch] loss: 2.41173, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:35:52.520922 Training: [20 epoch, 370 batch] loss: 2.43531, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:36:01.213941 Training: [20 epoch, 380 batch] loss: 2.52230, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "<Test> RMSE: 0.42989, MAE: 0.31418 \n",
            "2022-08-03 17:36:48.116435 Training: [21 epoch,  10 batch] loss: 2.43190, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:36:57.536634 Training: [21 epoch,  20 batch] loss: 2.45240, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:37:06.109164 Training: [21 epoch,  30 batch] loss: 2.44835, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:37:14.744158 Training: [21 epoch,  40 batch] loss: 2.44451, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:37:23.345001 Training: [21 epoch,  50 batch] loss: 2.44236, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:37:31.931238 Training: [21 epoch,  60 batch] loss: 2.44909, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:37:40.583269 Training: [21 epoch,  70 batch] loss: 2.51287, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:37:49.235339 Training: [21 epoch,  80 batch] loss: 2.48899, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:37:57.974361 Training: [21 epoch,  90 batch] loss: 2.45523, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:38:06.588900 Training: [21 epoch, 100 batch] loss: 2.46923, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:38:15.249434 Training: [21 epoch, 110 batch] loss: 2.46713, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:38:23.853243 Training: [21 epoch, 120 batch] loss: 2.47210, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:38:32.535921 Training: [21 epoch, 130 batch] loss: 2.45427, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:38:41.186068 Training: [21 epoch, 140 batch] loss: 2.46588, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:38:49.855247 Training: [21 epoch, 150 batch] loss: 2.42469, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:38:58.493194 Training: [21 epoch, 160 batch] loss: 2.53586, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:39:07.983972 Training: [21 epoch, 170 batch] loss: 2.47836, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:39:16.573660 Training: [21 epoch, 180 batch] loss: 2.45511, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:39:25.216432 Training: [21 epoch, 190 batch] loss: 2.41357, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:39:33.905672 Training: [21 epoch, 200 batch] loss: 2.43577, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:39:42.592693 Training: [21 epoch, 210 batch] loss: 2.44345, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:39:51.214844 Training: [21 epoch, 220 batch] loss: 2.43621, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:39:59.924799 Training: [21 epoch, 230 batch] loss: 2.41933, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:40:08.642954 Training: [21 epoch, 240 batch] loss: 2.46089, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:40:17.353628 Training: [21 epoch, 250 batch] loss: 2.43591, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:40:26.021468 Training: [21 epoch, 260 batch] loss: 2.47838, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:40:34.676790 Training: [21 epoch, 270 batch] loss: 2.46562, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:40:43.390466 Training: [21 epoch, 280 batch] loss: 2.45871, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:40:52.089528 Training: [21 epoch, 290 batch] loss: 2.46790, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:41:00.767760 Training: [21 epoch, 300 batch] loss: 2.42811, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:41:09.979115 Training: [21 epoch, 310 batch] loss: 2.64143, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:41:18.971056 Training: [21 epoch, 320 batch] loss: 2.44479, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:41:27.613776 Training: [21 epoch, 330 batch] loss: 2.50434, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:41:36.240697 Training: [21 epoch, 340 batch] loss: 2.47349, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:41:44.906906 Training: [21 epoch, 350 batch] loss: 2.45537, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:41:53.520894 Training: [21 epoch, 360 batch] loss: 2.53517, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:42:02.192177 Training: [21 epoch, 370 batch] loss: 2.44973, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:42:10.877284 Training: [21 epoch, 380 batch] loss: 2.43904, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "<Test> RMSE: 0.43730, MAE: 0.32860 \n",
            "2022-08-03 17:42:57.679850 Training: [22 epoch,  10 batch] loss: 2.43310, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:43:06.259468 Training: [22 epoch,  20 batch] loss: 2.43704, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:43:14.928668 Training: [22 epoch,  30 batch] loss: 2.43022, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:43:24.388287 Training: [22 epoch,  40 batch] loss: 2.48084, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:43:32.911266 Training: [22 epoch,  50 batch] loss: 2.47237, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:43:41.559154 Training: [22 epoch,  60 batch] loss: 2.43393, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:43:50.127526 Training: [22 epoch,  70 batch] loss: 2.45898, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:43:58.792244 Training: [22 epoch,  80 batch] loss: 2.62949, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:44:07.476041 Training: [22 epoch,  90 batch] loss: 2.41712, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:44:16.077857 Training: [22 epoch, 100 batch] loss: 2.45062, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:44:24.711139 Training: [22 epoch, 110 batch] loss: 2.44827, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:44:33.331457 Training: [22 epoch, 120 batch] loss: 2.44621, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:44:41.919800 Training: [22 epoch, 130 batch] loss: 2.47879, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:44:50.600758 Training: [22 epoch, 140 batch] loss: 2.42758, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:44:59.267294 Training: [22 epoch, 150 batch] loss: 2.41563, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:45:07.997094 Training: [22 epoch, 160 batch] loss: 2.47710, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:45:16.655084 Training: [22 epoch, 170 batch] loss: 2.45317, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:45:25.331837 Training: [22 epoch, 180 batch] loss: 2.51310, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:45:34.850427 Training: [22 epoch, 190 batch] loss: 2.42431, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:45:43.515937 Training: [22 epoch, 200 batch] loss: 2.46573, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:45:52.179288 Training: [22 epoch, 210 batch] loss: 2.44931, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:46:00.811134 Training: [22 epoch, 220 batch] loss: 2.43703, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:46:09.518945 Training: [22 epoch, 230 batch] loss: 2.45299, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:46:18.177245 Training: [22 epoch, 240 batch] loss: 2.42016, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:46:26.847317 Training: [22 epoch, 250 batch] loss: 2.42596, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:46:35.513419 Training: [22 epoch, 260 batch] loss: 2.44202, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:46:44.282419 Training: [22 epoch, 270 batch] loss: 2.58513, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:46:52.967287 Training: [22 epoch, 280 batch] loss: 2.43305, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:47:01.621862 Training: [22 epoch, 290 batch] loss: 2.47267, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:47:10.274742 Training: [22 epoch, 300 batch] loss: 2.53316, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:47:18.933561 Training: [22 epoch, 310 batch] loss: 2.54107, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:47:27.604558 Training: [22 epoch, 320 batch] loss: 2.45946, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:47:37.169672 Training: [22 epoch, 330 batch] loss: 2.43778, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:47:45.851938 Training: [22 epoch, 340 batch] loss: 2.44913, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:47:54.537452 Training: [22 epoch, 350 batch] loss: 2.45727, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:48:03.237699 Training: [22 epoch, 360 batch] loss: 2.42844, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:48:11.924707 Training: [22 epoch, 370 batch] loss: 2.44675, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:48:20.554567 Training: [22 epoch, 380 batch] loss: 2.55497, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "<Test> RMSE: 0.45432, MAE: 0.35867 \n",
            "2022-08-03 17:49:07.329488 Training: [23 epoch,  10 batch] loss: 2.41942, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:49:16.030840 Training: [23 epoch,  20 batch] loss: 2.46317, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:49:24.688238 Training: [23 epoch,  30 batch] loss: 2.43017, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:49:33.293372 Training: [23 epoch,  40 batch] loss: 2.44266, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:49:41.853568 Training: [23 epoch,  50 batch] loss: 2.46295, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:49:51.365828 Training: [23 epoch,  60 batch] loss: 2.44034, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:50:00.057515 Training: [23 epoch,  70 batch] loss: 2.45506, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:50:08.690851 Training: [23 epoch,  80 batch] loss: 2.47788, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:50:17.347227 Training: [23 epoch,  90 batch] loss: 2.45613, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:50:26.066679 Training: [23 epoch, 100 batch] loss: 2.46181, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:50:34.737012 Training: [23 epoch, 110 batch] loss: 2.46494, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:50:43.447453 Training: [23 epoch, 120 batch] loss: 2.44681, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:50:52.051840 Training: [23 epoch, 130 batch] loss: 2.56631, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:51:00.731903 Training: [23 epoch, 140 batch] loss: 2.44448, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:51:09.349738 Training: [23 epoch, 150 batch] loss: 2.62085, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:51:18.021031 Training: [23 epoch, 160 batch] loss: 2.44635, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:51:26.642497 Training: [23 epoch, 170 batch] loss: 2.45505, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:51:35.350162 Training: [23 epoch, 180 batch] loss: 2.45131, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:51:43.989886 Training: [23 epoch, 190 batch] loss: 2.50157, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:51:53.555278 Training: [23 epoch, 200 batch] loss: 2.54842, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:52:02.215756 Training: [23 epoch, 210 batch] loss: 2.47948, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:52:10.928888 Training: [23 epoch, 220 batch] loss: 2.42453, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:52:19.612136 Training: [23 epoch, 230 batch] loss: 2.45905, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:52:28.265524 Training: [23 epoch, 240 batch] loss: 2.47713, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:52:36.929941 Training: [23 epoch, 250 batch] loss: 2.43525, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:52:45.575198 Training: [23 epoch, 260 batch] loss: 2.44029, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:52:54.251588 Training: [23 epoch, 270 batch] loss: 2.55008, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:53:02.921520 Training: [23 epoch, 280 batch] loss: 2.44309, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:53:11.585360 Training: [23 epoch, 290 batch] loss: 2.42919, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:53:20.248073 Training: [23 epoch, 300 batch] loss: 2.45399, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:53:28.983001 Training: [23 epoch, 310 batch] loss: 2.46578, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:53:37.633539 Training: [23 epoch, 320 batch] loss: 2.41076, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:53:46.235341 Training: [23 epoch, 330 batch] loss: 2.45625, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:53:54.898857 Training: [23 epoch, 340 batch] loss: 2.44502, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:54:04.433608 Training: [23 epoch, 350 batch] loss: 2.48297, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:54:13.113092 Training: [23 epoch, 360 batch] loss: 2.42687, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:54:21.737986 Training: [23 epoch, 370 batch] loss: 2.43392, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:54:30.384247 Training: [23 epoch, 380 batch] loss: 2.41361, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "<Test> RMSE: 0.43438, MAE: 0.32303 \n",
            "2022-08-03 17:55:17.228317 Training: [24 epoch,  10 batch] loss: 2.47225, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:55:25.844284 Training: [24 epoch,  20 batch] loss: 2.43521, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:55:34.467329 Training: [24 epoch,  30 batch] loss: 2.42312, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:55:43.064834 Training: [24 epoch,  40 batch] loss: 2.45425, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:55:51.656031 Training: [24 epoch,  50 batch] loss: 2.43768, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:56:00.288425 Training: [24 epoch,  60 batch] loss: 2.41901, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:56:09.605012 Training: [24 epoch,  70 batch] loss: 2.43639, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:56:18.448300 Training: [24 epoch,  80 batch] loss: 2.44398, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:56:27.105420 Training: [24 epoch,  90 batch] loss: 2.50278, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:56:35.753065 Training: [24 epoch, 100 batch] loss: 2.46336, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:56:44.425214 Training: [24 epoch, 110 batch] loss: 2.41707, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:56:53.029563 Training: [24 epoch, 120 batch] loss: 2.43197, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:57:01.694640 Training: [24 epoch, 130 batch] loss: 2.66654, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:57:10.432200 Training: [24 epoch, 140 batch] loss: 2.41873, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:57:19.138171 Training: [24 epoch, 150 batch] loss: 2.42567, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:57:27.812844 Training: [24 epoch, 160 batch] loss: 2.47590, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:57:36.507282 Training: [24 epoch, 170 batch] loss: 2.45341, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:57:45.187240 Training: [24 epoch, 180 batch] loss: 2.49946, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:57:53.835665 Training: [24 epoch, 190 batch] loss: 2.53856, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:58:02.468406 Training: [24 epoch, 200 batch] loss: 2.44019, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:58:11.101210 Training: [24 epoch, 210 batch] loss: 2.49486, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:58:20.427726 Training: [24 epoch, 220 batch] loss: 2.46109, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:58:29.069202 Training: [24 epoch, 230 batch] loss: 2.45801, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:58:37.669430 Training: [24 epoch, 240 batch] loss: 2.47374, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:58:46.334806 Training: [24 epoch, 250 batch] loss: 2.43816, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:58:54.990563 Training: [24 epoch, 260 batch] loss: 2.45768, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:59:03.679130 Training: [24 epoch, 270 batch] loss: 2.52056, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:59:12.325127 Training: [24 epoch, 280 batch] loss: 2.45097, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:59:20.983581 Training: [24 epoch, 290 batch] loss: 2.45240, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:59:29.606625 Training: [24 epoch, 300 batch] loss: 2.45572, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:59:38.259128 Training: [24 epoch, 310 batch] loss: 2.45974, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:59:46.929778 Training: [24 epoch, 320 batch] loss: 2.44501, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:59:55.604700 Training: [24 epoch, 330 batch] loss: 2.47077, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:00:04.311983 Training: [24 epoch, 340 batch] loss: 2.42630, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:00:13.046400 Training: [24 epoch, 350 batch] loss: 2.40884, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:00:21.809883 Training: [24 epoch, 360 batch] loss: 2.45792, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:00:31.112128 Training: [24 epoch, 370 batch] loss: 2.46379, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:00:39.824444 Training: [24 epoch, 380 batch] loss: 2.53519, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "<Test> RMSE: 0.45779, MAE: 0.36440 \n",
            "2022-08-03 18:01:26.396349 Training: [25 epoch,  10 batch] loss: 2.50871, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:01:34.992066 Training: [25 epoch,  20 batch] loss: 2.44100, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:01:43.552890 Training: [25 epoch,  30 batch] loss: 2.44538, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:01:52.134215 Training: [25 epoch,  40 batch] loss: 2.46174, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:02:00.667009 Training: [25 epoch,  50 batch] loss: 2.46391, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:02:09.240136 Training: [25 epoch,  60 batch] loss: 2.44462, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:02:17.806307 Training: [25 epoch,  70 batch] loss: 2.47016, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:02:26.427787 Training: [25 epoch,  80 batch] loss: 2.64552, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:02:35.800489 Training: [25 epoch,  90 batch] loss: 2.42920, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:02:44.480927 Training: [25 epoch, 100 batch] loss: 2.47322, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:02:53.091533 Training: [25 epoch, 110 batch] loss: 2.46932, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:03:01.812216 Training: [25 epoch, 120 batch] loss: 2.43933, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:03:10.467217 Training: [25 epoch, 130 batch] loss: 2.46461, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:03:19.090998 Training: [25 epoch, 140 batch] loss: 2.44595, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:03:27.649219 Training: [25 epoch, 150 batch] loss: 2.44528, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:03:36.297922 Training: [25 epoch, 160 batch] loss: 2.51089, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:03:44.947787 Training: [25 epoch, 170 batch] loss: 2.46810, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:03:53.602259 Training: [25 epoch, 180 batch] loss: 2.44203, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:04:02.245236 Training: [25 epoch, 190 batch] loss: 2.42221, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:04:10.913129 Training: [25 epoch, 200 batch] loss: 2.50477, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:04:19.596353 Training: [25 epoch, 210 batch] loss: 2.46049, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:04:28.217429 Training: [25 epoch, 220 batch] loss: 2.43518, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:04:37.534962 Training: [25 epoch, 230 batch] loss: 2.41628, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:04:46.171114 Training: [25 epoch, 240 batch] loss: 2.41975, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:04:54.816276 Training: [25 epoch, 250 batch] loss: 2.42426, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:05:03.479084 Training: [25 epoch, 260 batch] loss: 2.45118, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:05:12.066908 Training: [25 epoch, 270 batch] loss: 2.46789, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:05:20.720451 Training: [25 epoch, 280 batch] loss: 2.47033, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:05:29.418831 Training: [25 epoch, 290 batch] loss: 2.49764, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:05:38.077925 Training: [25 epoch, 300 batch] loss: 2.46068, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:05:46.698775 Training: [25 epoch, 310 batch] loss: 2.43838, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:05:55.354650 Training: [25 epoch, 320 batch] loss: 2.46269, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:06:04.003880 Training: [25 epoch, 330 batch] loss: 2.45224, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:06:12.693202 Training: [25 epoch, 340 batch] loss: 2.45303, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:06:21.304832 Training: [25 epoch, 350 batch] loss: 2.45270, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:06:29.901536 Training: [25 epoch, 360 batch] loss: 2.41641, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:06:39.265359 Training: [25 epoch, 370 batch] loss: 2.42513, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:06:47.941177 Training: [25 epoch, 380 batch] loss: 2.51365, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "<Test> RMSE: 0.50495, MAE: 0.43406 \n",
            "2022-08-03 18:07:34.405617 Training: [26 epoch,  10 batch] loss: 2.44813, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:07:42.951245 Training: [26 epoch,  20 batch] loss: 2.42044, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:07:51.507089 Training: [26 epoch,  30 batch] loss: 2.46784, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:08:00.134849 Training: [26 epoch,  40 batch] loss: 2.60383, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:08:08.768731 Training: [26 epoch,  50 batch] loss: 2.45290, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:08:17.328350 Training: [26 epoch,  60 batch] loss: 2.43449, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:08:25.914486 Training: [26 epoch,  70 batch] loss: 2.42991, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:08:34.474735 Training: [26 epoch,  80 batch] loss: 2.46413, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:08:43.852308 Training: [26 epoch,  90 batch] loss: 2.45344, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:08:52.434863 Training: [26 epoch, 100 batch] loss: 2.50278, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:09:01.023514 Training: [26 epoch, 110 batch] loss: 2.45264, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:09:09.656399 Training: [26 epoch, 120 batch] loss: 2.44368, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:09:18.287018 Training: [26 epoch, 130 batch] loss: 2.46890, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:09:26.905019 Training: [26 epoch, 140 batch] loss: 2.45329, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:09:35.597410 Training: [26 epoch, 150 batch] loss: 2.44575, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:09:44.238354 Training: [26 epoch, 160 batch] loss: 2.47990, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:09:52.874082 Training: [26 epoch, 170 batch] loss: 2.42634, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:10:01.468730 Training: [26 epoch, 180 batch] loss: 2.43390, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:10:10.121318 Training: [26 epoch, 190 batch] loss: 2.47227, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:10:18.769421 Training: [26 epoch, 200 batch] loss: 2.52206, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:10:27.453310 Training: [26 epoch, 210 batch] loss: 2.51085, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:10:36.060829 Training: [26 epoch, 220 batch] loss: 2.44181, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:10:44.813656 Training: [26 epoch, 230 batch] loss: 2.46144, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:10:54.192397 Training: [26 epoch, 240 batch] loss: 2.44007, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:11:02.896845 Training: [26 epoch, 250 batch] loss: 2.43778, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:11:11.513861 Training: [26 epoch, 260 batch] loss: 2.48113, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:11:20.213899 Training: [26 epoch, 270 batch] loss: 2.45363, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:11:28.908029 Training: [26 epoch, 280 batch] loss: 2.45563, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:11:37.563096 Training: [26 epoch, 290 batch] loss: 2.47943, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:11:46.217237 Training: [26 epoch, 300 batch] loss: 2.46294, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:11:54.851578 Training: [26 epoch, 310 batch] loss: 2.41796, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:12:03.443196 Training: [26 epoch, 320 batch] loss: 2.43418, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:12:12.132930 Training: [26 epoch, 330 batch] loss: 2.41632, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:12:20.790557 Training: [26 epoch, 340 batch] loss: 2.42494, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:12:29.471413 Training: [26 epoch, 350 batch] loss: 2.43451, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:12:38.177615 Training: [26 epoch, 360 batch] loss: 2.52483, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:12:46.855080 Training: [26 epoch, 370 batch] loss: 2.47621, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:12:56.164685 Training: [26 epoch, 380 batch] loss: 2.45961, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "<Test> RMSE: 0.45987, MAE: 0.36777 \n",
            "2022-08-03 18:13:42.639828 Training: [27 epoch,  10 batch] loss: 2.44258, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:13:51.256454 Training: [27 epoch,  20 batch] loss: 2.44636, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:13:59.856708 Training: [27 epoch,  30 batch] loss: 2.48699, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:14:08.426457 Training: [27 epoch,  40 batch] loss: 2.44308, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:14:17.052179 Training: [27 epoch,  50 batch] loss: 2.44345, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:14:25.666869 Training: [27 epoch,  60 batch] loss: 2.42706, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:14:34.251664 Training: [27 epoch,  70 batch] loss: 2.44829, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:14:42.860564 Training: [27 epoch,  80 batch] loss: 2.45735, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:14:51.479542 Training: [27 epoch,  90 batch] loss: 2.42857, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:15:00.822981 Training: [27 epoch, 100 batch] loss: 2.46339, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:15:09.418492 Training: [27 epoch, 110 batch] loss: 2.46829, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:15:18.005447 Training: [27 epoch, 120 batch] loss: 2.47736, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:15:26.597191 Training: [27 epoch, 130 batch] loss: 2.41617, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:15:35.205593 Training: [27 epoch, 140 batch] loss: 2.42452, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:15:43.819022 Training: [27 epoch, 150 batch] loss: 2.43913, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:15:52.471367 Training: [27 epoch, 160 batch] loss: 2.43597, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:16:01.097584 Training: [27 epoch, 170 batch] loss: 2.43149, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:16:09.762187 Training: [27 epoch, 180 batch] loss: 2.47633, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:16:18.504693 Training: [27 epoch, 190 batch] loss: 2.51186, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:16:27.142452 Training: [27 epoch, 200 batch] loss: 2.43083, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:16:35.750887 Training: [27 epoch, 210 batch] loss: 2.50853, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:16:44.393948 Training: [27 epoch, 220 batch] loss: 2.45304, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:16:53.056558 Training: [27 epoch, 230 batch] loss: 2.45372, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:17:02.273896 Training: [27 epoch, 240 batch] loss: 2.49053, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:17:11.006767 Training: [27 epoch, 250 batch] loss: 2.43463, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:17:19.627434 Training: [27 epoch, 260 batch] loss: 2.46821, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:17:28.351045 Training: [27 epoch, 270 batch] loss: 2.42953, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:17:37.047847 Training: [27 epoch, 280 batch] loss: 2.50089, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:17:45.724341 Training: [27 epoch, 290 batch] loss: 2.49256, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:17:54.383896 Training: [27 epoch, 300 batch] loss: 2.42298, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:18:03.050372 Training: [27 epoch, 310 batch] loss: 2.45897, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:18:11.723706 Training: [27 epoch, 320 batch] loss: 2.46558, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:18:20.315221 Training: [27 epoch, 330 batch] loss: 2.49973, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:18:28.968940 Training: [27 epoch, 340 batch] loss: 2.61534, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:18:37.703486 Training: [27 epoch, 350 batch] loss: 2.47796, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:18:46.391136 Training: [27 epoch, 360 batch] loss: 2.44935, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:18:55.045778 Training: [27 epoch, 370 batch] loss: 2.46034, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:19:03.700442 Training: [27 epoch, 380 batch] loss: 2.43914, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "<Test> RMSE: 0.48487, MAE: 0.40592 \n",
            "2022-08-03 18:19:50.871224 Training: [28 epoch,  10 batch] loss: 2.42046, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:19:59.421351 Training: [28 epoch,  20 batch] loss: 2.42340, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:20:07.996423 Training: [28 epoch,  30 batch] loss: 2.60735, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:20:16.615873 Training: [28 epoch,  40 batch] loss: 2.43821, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:20:25.195634 Training: [28 epoch,  50 batch] loss: 2.44459, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:20:33.837399 Training: [28 epoch,  60 batch] loss: 2.42766, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:20:42.440317 Training: [28 epoch,  70 batch] loss: 2.45026, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:20:51.037192 Training: [28 epoch,  80 batch] loss: 2.43136, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:20:59.640254 Training: [28 epoch,  90 batch] loss: 2.49226, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:21:08.484055 Training: [28 epoch, 100 batch] loss: 2.40575, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:21:17.640809 Training: [28 epoch, 110 batch] loss: 2.43485, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:21:26.310892 Training: [28 epoch, 120 batch] loss: 2.45616, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:21:35.001088 Training: [28 epoch, 130 batch] loss: 2.43521, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:21:43.604750 Training: [28 epoch, 140 batch] loss: 2.50745, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:21:52.268407 Training: [28 epoch, 150 batch] loss: 2.42442, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:22:00.813036 Training: [28 epoch, 160 batch] loss: 2.44918, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:22:09.442353 Training: [28 epoch, 170 batch] loss: 2.43758, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:22:18.119203 Training: [28 epoch, 180 batch] loss: 2.46218, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:22:26.739304 Training: [28 epoch, 190 batch] loss: 2.51691, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:22:35.364581 Training: [28 epoch, 200 batch] loss: 2.48229, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:22:44.010284 Training: [28 epoch, 210 batch] loss: 2.44437, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:22:52.677643 Training: [28 epoch, 220 batch] loss: 2.47581, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:23:01.397613 Training: [28 epoch, 230 batch] loss: 2.43561, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:23:10.315177 Training: [28 epoch, 240 batch] loss: 2.51590, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:23:19.377108 Training: [28 epoch, 250 batch] loss: 2.43823, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:23:28.044460 Training: [28 epoch, 260 batch] loss: 2.46255, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:23:36.685605 Training: [28 epoch, 270 batch] loss: 2.47065, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:23:45.315674 Training: [28 epoch, 280 batch] loss: 2.47971, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:23:53.948618 Training: [28 epoch, 290 batch] loss: 2.44234, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:24:02.574644 Training: [28 epoch, 300 batch] loss: 2.48439, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:24:11.193877 Training: [28 epoch, 310 batch] loss: 2.43031, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:24:19.816963 Training: [28 epoch, 320 batch] loss: 2.44592, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:24:28.503435 Training: [28 epoch, 330 batch] loss: 2.53798, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:24:37.125694 Training: [28 epoch, 340 batch] loss: 2.44089, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:24:45.737580 Training: [28 epoch, 350 batch] loss: 2.44965, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:24:54.294640 Training: [28 epoch, 360 batch] loss: 2.42619, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:25:02.988864 Training: [28 epoch, 370 batch] loss: 2.43266, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:25:11.642900 Training: [28 epoch, 380 batch] loss: 2.51238, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "<Test> RMSE: 0.44421, MAE: 0.34120 \n",
            "2022-08-03 18:25:58.873316 Training: [29 epoch,  10 batch] loss: 2.42648, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:26:07.439267 Training: [29 epoch,  20 batch] loss: 2.40309, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:26:15.943683 Training: [29 epoch,  30 batch] loss: 2.44478, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:26:24.564913 Training: [29 epoch,  40 batch] loss: 2.47250, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:26:33.207298 Training: [29 epoch,  50 batch] loss: 2.44082, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:26:41.871646 Training: [29 epoch,  60 batch] loss: 2.45093, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:26:50.446382 Training: [29 epoch,  70 batch] loss: 2.44266, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:26:59.081942 Training: [29 epoch,  80 batch] loss: 2.55291, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:27:07.733458 Training: [29 epoch,  90 batch] loss: 2.42177, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:27:16.557525 Training: [29 epoch, 100 batch] loss: 2.44247, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:27:25.702421 Training: [29 epoch, 110 batch] loss: 2.45429, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:27:34.390818 Training: [29 epoch, 120 batch] loss: 2.45730, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:27:43.054017 Training: [29 epoch, 130 batch] loss: 2.44331, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:27:51.751090 Training: [29 epoch, 140 batch] loss: 2.45527, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:28:00.401788 Training: [29 epoch, 150 batch] loss: 2.51902, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:28:09.104324 Training: [29 epoch, 160 batch] loss: 2.43753, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:28:17.851351 Training: [29 epoch, 170 batch] loss: 2.44710, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:28:26.537903 Training: [29 epoch, 180 batch] loss: 2.43760, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:28:35.152006 Training: [29 epoch, 190 batch] loss: 2.59520, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:28:43.850427 Training: [29 epoch, 200 batch] loss: 2.47610, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:28:52.489828 Training: [29 epoch, 210 batch] loss: 2.43635, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:29:01.141559 Training: [29 epoch, 220 batch] loss: 2.49612, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:29:09.856540 Training: [29 epoch, 230 batch] loss: 2.42204, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:29:19.182132 Training: [29 epoch, 240 batch] loss: 2.44030, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:29:27.978200 Training: [29 epoch, 250 batch] loss: 2.47093, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:29:36.738128 Training: [29 epoch, 260 batch] loss: 2.55594, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:29:45.493206 Training: [29 epoch, 270 batch] loss: 2.42218, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:29:54.235493 Training: [29 epoch, 280 batch] loss: 2.44482, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:30:02.956411 Training: [29 epoch, 290 batch] loss: 2.48822, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:30:11.715205 Training: [29 epoch, 300 batch] loss: 2.43632, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:30:20.470789 Training: [29 epoch, 310 batch] loss: 2.42769, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:30:29.107045 Training: [29 epoch, 320 batch] loss: 2.43044, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:30:37.818182 Training: [29 epoch, 330 batch] loss: 2.45417, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:30:46.515229 Training: [29 epoch, 340 batch] loss: 2.45987, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:30:55.214670 Training: [29 epoch, 350 batch] loss: 2.40644, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:31:03.868638 Training: [29 epoch, 360 batch] loss: 2.43341, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:31:12.558262 Training: [29 epoch, 370 batch] loss: 2.43059, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:31:21.952446 Training: [29 epoch, 380 batch] loss: 2.51338, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "<Test> RMSE: 0.50355, MAE: 0.43213 \n",
            "2022-08-03 18:32:08.782766 Training: [30 epoch,  10 batch] loss: 2.47065, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:32:17.393870 Training: [30 epoch,  20 batch] loss: 2.49418, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:32:26.051631 Training: [30 epoch,  30 batch] loss: 2.46629, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:32:34.686748 Training: [30 epoch,  40 batch] loss: 2.43198, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:32:43.322315 Training: [30 epoch,  50 batch] loss: 2.53684, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:32:51.902982 Training: [30 epoch,  60 batch] loss: 2.48616, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:33:00.603033 Training: [30 epoch,  70 batch] loss: 2.48165, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:33:09.311719 Training: [30 epoch,  80 batch] loss: 2.44388, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:33:17.990867 Training: [30 epoch,  90 batch] loss: 2.42357, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:33:27.298671 Training: [30 epoch, 100 batch] loss: 2.50200, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:33:35.921180 Training: [30 epoch, 110 batch] loss: 2.45057, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:33:44.665737 Training: [30 epoch, 120 batch] loss: 2.43976, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:33:53.352262 Training: [30 epoch, 130 batch] loss: 2.45435, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:34:02.035413 Training: [30 epoch, 140 batch] loss: 2.41755, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:34:10.685185 Training: [30 epoch, 150 batch] loss: 2.46817, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:34:19.451366 Training: [30 epoch, 160 batch] loss: 2.41830, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:34:28.121514 Training: [30 epoch, 170 batch] loss: 2.45500, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:34:36.824014 Training: [30 epoch, 180 batch] loss: 2.44748, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:34:45.516656 Training: [30 epoch, 190 batch] loss: 2.44651, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:34:54.262344 Training: [30 epoch, 200 batch] loss: 2.46756, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:35:02.994928 Training: [30 epoch, 210 batch] loss: 2.48172, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:35:11.717045 Training: [30 epoch, 220 batch] loss: 2.48335, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:35:20.411655 Training: [30 epoch, 230 batch] loss: 2.44878, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:35:29.827903 Training: [30 epoch, 240 batch] loss: 2.46126, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:35:38.524722 Training: [30 epoch, 250 batch] loss: 2.41696, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:35:47.267453 Training: [30 epoch, 260 batch] loss: 2.41446, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:35:55.937378 Training: [30 epoch, 270 batch] loss: 2.43455, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:36:04.624877 Training: [30 epoch, 280 batch] loss: 2.42547, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:36:13.302390 Training: [30 epoch, 290 batch] loss: 2.45251, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:36:22.039459 Training: [30 epoch, 300 batch] loss: 2.42863, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:36:30.702563 Training: [30 epoch, 310 batch] loss: 2.44392, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:36:39.328218 Training: [30 epoch, 320 batch] loss: 2.46649, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:36:48.024203 Training: [30 epoch, 330 batch] loss: 2.48371, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:36:56.770756 Training: [30 epoch, 340 batch] loss: 2.48403, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:37:05.457640 Training: [30 epoch, 350 batch] loss: 2.42824, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:37:14.194137 Training: [30 epoch, 360 batch] loss: 2.42835, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:37:23.496699 Training: [30 epoch, 370 batch] loss: 2.60549, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:37:32.405037 Training: [30 epoch, 380 batch] loss: 2.41025, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "<Test> RMSE: 0.60575, MAE: 0.55740 \n",
            "2022-08-03 18:38:19.025872 Training: [31 epoch,  10 batch] loss: 2.40588, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:38:27.624363 Training: [31 epoch,  20 batch] loss: 2.43314, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:38:36.235391 Training: [31 epoch,  30 batch] loss: 2.55606, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:38:44.874810 Training: [31 epoch,  40 batch] loss: 2.42880, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:38:53.533956 Training: [31 epoch,  50 batch] loss: 2.45072, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:39:02.132338 Training: [31 epoch,  60 batch] loss: 2.43349, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:39:10.775703 Training: [31 epoch,  70 batch] loss: 2.43258, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:39:19.392271 Training: [31 epoch,  80 batch] loss: 2.47809, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:39:28.735430 Training: [31 epoch,  90 batch] loss: 2.51355, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:39:37.386039 Training: [31 epoch, 100 batch] loss: 2.44959, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:39:46.056536 Training: [31 epoch, 110 batch] loss: 2.49175, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:39:54.676701 Training: [31 epoch, 120 batch] loss: 2.50192, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:40:03.344695 Training: [31 epoch, 130 batch] loss: 2.41132, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:40:11.947445 Training: [31 epoch, 140 batch] loss: 2.40633, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:40:20.670251 Training: [31 epoch, 150 batch] loss: 2.45975, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:40:29.396170 Training: [31 epoch, 160 batch] loss: 2.45791, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:40:38.115984 Training: [31 epoch, 170 batch] loss: 2.47779, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:40:46.718561 Training: [31 epoch, 180 batch] loss: 2.45334, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:40:55.346031 Training: [31 epoch, 190 batch] loss: 2.44162, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:41:03.993617 Training: [31 epoch, 200 batch] loss: 2.57213, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:41:12.649570 Training: [31 epoch, 210 batch] loss: 2.45467, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:41:21.600317 Training: [31 epoch, 220 batch] loss: 2.47021, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:41:30.761817 Training: [31 epoch, 230 batch] loss: 2.44687, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:41:39.476238 Training: [31 epoch, 240 batch] loss: 2.43525, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:41:48.199899 Training: [31 epoch, 250 batch] loss: 2.44861, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:41:56.854357 Training: [31 epoch, 260 batch] loss: 2.40768, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:42:05.575715 Training: [31 epoch, 270 batch] loss: 2.42659, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:42:14.306321 Training: [31 epoch, 280 batch] loss: 2.43913, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:42:22.971735 Training: [31 epoch, 290 batch] loss: 2.41990, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:42:31.651716 Training: [31 epoch, 300 batch] loss: 2.44195, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:42:40.333296 Training: [31 epoch, 310 batch] loss: 2.48809, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:42:48.979345 Training: [31 epoch, 320 batch] loss: 2.60168, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:42:57.668366 Training: [31 epoch, 330 batch] loss: 2.44039, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:43:06.366452 Training: [31 epoch, 340 batch] loss: 2.47637, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:43:15.077914 Training: [31 epoch, 350 batch] loss: 2.44280, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:43:24.494273 Training: [31 epoch, 360 batch] loss: 2.46118, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:43:33.166981 Training: [31 epoch, 370 batch] loss: 2.44678, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:43:41.825362 Training: [31 epoch, 380 batch] loss: 2.43678, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "<Test> RMSE: 0.61866, MAE: 0.57190 \n",
            "2022-08-03 18:44:28.608301 Training: [32 epoch,  10 batch] loss: 2.49975, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:44:37.249806 Training: [32 epoch,  20 batch] loss: 2.40905, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:44:45.847317 Training: [32 epoch,  30 batch] loss: 2.65066, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:44:54.505532 Training: [32 epoch,  40 batch] loss: 2.45189, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:45:03.091301 Training: [32 epoch,  50 batch] loss: 2.41379, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:45:11.726136 Training: [32 epoch,  60 batch] loss: 2.47917, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:45:21.049158 Training: [32 epoch,  70 batch] loss: 2.45997, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:45:29.745675 Training: [32 epoch,  80 batch] loss: 2.48387, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:45:38.409696 Training: [32 epoch,  90 batch] loss: 2.44734, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:45:47.133576 Training: [32 epoch, 100 batch] loss: 2.45548, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:45:55.800939 Training: [32 epoch, 110 batch] loss: 2.42335, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:46:04.457118 Training: [32 epoch, 120 batch] loss: 2.42130, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:46:13.109236 Training: [32 epoch, 130 batch] loss: 2.43821, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:46:21.798094 Training: [32 epoch, 140 batch] loss: 2.44079, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:46:30.484782 Training: [32 epoch, 150 batch] loss: 2.51268, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:46:39.146887 Training: [32 epoch, 160 batch] loss: 2.43768, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:46:47.742347 Training: [32 epoch, 170 batch] loss: 2.43856, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:46:56.445361 Training: [32 epoch, 180 batch] loss: 2.41936, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:47:05.157122 Training: [32 epoch, 190 batch] loss: 2.44649, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:47:13.859352 Training: [32 epoch, 200 batch] loss: 2.51687, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:47:23.208856 Training: [32 epoch, 210 batch] loss: 2.48273, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:47:31.933948 Training: [32 epoch, 220 batch] loss: 2.44679, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:47:40.682188 Training: [32 epoch, 230 batch] loss: 2.45873, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:47:49.326104 Training: [32 epoch, 240 batch] loss: 2.43749, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:47:57.986322 Training: [32 epoch, 250 batch] loss: 2.43317, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:48:06.629892 Training: [32 epoch, 260 batch] loss: 2.48217, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:48:15.395757 Training: [32 epoch, 270 batch] loss: 2.46169, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:48:24.123139 Training: [32 epoch, 280 batch] loss: 2.46368, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:48:32.858306 Training: [32 epoch, 290 batch] loss: 2.51593, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:48:41.638820 Training: [32 epoch, 300 batch] loss: 2.43614, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:48:50.326727 Training: [32 epoch, 310 batch] loss: 2.44541, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:48:59.036816 Training: [32 epoch, 320 batch] loss: 2.42231, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:49:07.713461 Training: [32 epoch, 330 batch] loss: 2.43053, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:49:16.558815 Training: [32 epoch, 340 batch] loss: 2.44226, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:49:25.824346 Training: [32 epoch, 350 batch] loss: 2.43021, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:49:34.460435 Training: [32 epoch, 360 batch] loss: 2.40773, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:49:43.109378 Training: [32 epoch, 370 batch] loss: 2.45353, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:49:51.824884 Training: [32 epoch, 380 batch] loss: 2.47247, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "<Test> RMSE: 0.55263, MAE: 0.49515 \n",
            "2022-08-03 18:50:38.828329 Training: [33 epoch,  10 batch] loss: 2.49363, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:50:47.409901 Training: [33 epoch,  20 batch] loss: 2.41817, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:50:56.033206 Training: [33 epoch,  30 batch] loss: 2.45773, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:51:04.632221 Training: [33 epoch,  40 batch] loss: 2.43504, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:51:13.279269 Training: [33 epoch,  50 batch] loss: 2.41863, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:51:22.578302 Training: [33 epoch,  60 batch] loss: 2.45014, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:51:31.270509 Training: [33 epoch,  70 batch] loss: 2.43936, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:51:39.914995 Training: [33 epoch,  80 batch] loss: 2.44130, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:51:48.621988 Training: [33 epoch,  90 batch] loss: 2.44285, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:51:57.330761 Training: [33 epoch, 100 batch] loss: 2.47105, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:52:06.002779 Training: [33 epoch, 110 batch] loss: 2.48378, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:52:14.638502 Training: [33 epoch, 120 batch] loss: 2.51047, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:52:23.383254 Training: [33 epoch, 130 batch] loss: 2.44311, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:52:32.180838 Training: [33 epoch, 140 batch] loss: 2.41357, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:52:40.909022 Training: [33 epoch, 150 batch] loss: 2.42658, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:52:49.509501 Training: [33 epoch, 160 batch] loss: 2.43718, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:52:58.170811 Training: [33 epoch, 170 batch] loss: 2.45707, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:53:06.781620 Training: [33 epoch, 180 batch] loss: 2.44787, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:53:16.227243 Training: [33 epoch, 190 batch] loss: 2.45260, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:53:24.850430 Training: [33 epoch, 200 batch] loss: 2.48439, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:53:33.619716 Training: [33 epoch, 210 batch] loss: 2.41064, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:53:42.382912 Training: [33 epoch, 220 batch] loss: 2.47264, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:53:51.058005 Training: [33 epoch, 230 batch] loss: 2.51541, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:53:59.758144 Training: [33 epoch, 240 batch] loss: 2.46804, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:54:08.482979 Training: [33 epoch, 250 batch] loss: 2.45807, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:54:17.134565 Training: [33 epoch, 260 batch] loss: 2.43526, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:54:25.825925 Training: [33 epoch, 270 batch] loss: 2.43626, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:54:34.505529 Training: [33 epoch, 280 batch] loss: 2.45804, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:54:43.213575 Training: [33 epoch, 290 batch] loss: 2.43847, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:54:51.898696 Training: [33 epoch, 300 batch] loss: 2.62896, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:55:00.652057 Training: [33 epoch, 310 batch] loss: 2.47774, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:55:10.063999 Training: [33 epoch, 320 batch] loss: 2.45630, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:55:18.807968 Training: [33 epoch, 330 batch] loss: 2.43628, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:55:27.581987 Training: [33 epoch, 340 batch] loss: 2.44304, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:55:36.273831 Training: [33 epoch, 350 batch] loss: 2.41236, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:55:45.014864 Training: [33 epoch, 360 batch] loss: 2.44734, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:55:53.736665 Training: [33 epoch, 370 batch] loss: 2.43283, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:56:02.371229 Training: [33 epoch, 380 batch] loss: 2.51933, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "<Test> RMSE: 0.61558, MAE: 0.56845 \n",
            "2022-08-03 18:56:49.333912 Training: [34 epoch,  10 batch] loss: 2.42869, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:56:57.996131 Training: [34 epoch,  20 batch] loss: 2.43906, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:57:07.255247 Training: [34 epoch,  30 batch] loss: 2.41423, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:57:15.902553 Training: [34 epoch,  40 batch] loss: 2.44393, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:57:24.597766 Training: [34 epoch,  50 batch] loss: 2.44524, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:57:33.321305 Training: [34 epoch,  60 batch] loss: 2.43640, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:57:42.016048 Training: [34 epoch,  70 batch] loss: 2.59246, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:57:50.715504 Training: [34 epoch,  80 batch] loss: 2.48471, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:57:59.445569 Training: [34 epoch,  90 batch] loss: 2.44625, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:58:08.074940 Training: [34 epoch, 100 batch] loss: 2.47429, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:58:16.715969 Training: [34 epoch, 110 batch] loss: 2.45153, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:58:25.476589 Training: [34 epoch, 120 batch] loss: 2.45342, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:58:34.186454 Training: [34 epoch, 130 batch] loss: 2.44290, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:58:42.938865 Training: [34 epoch, 140 batch] loss: 2.42983, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:58:51.657594 Training: [34 epoch, 150 batch] loss: 2.44532, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:59:01.083863 Training: [34 epoch, 160 batch] loss: 2.42893, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:59:09.838061 Training: [34 epoch, 170 batch] loss: 2.42119, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:59:18.638067 Training: [34 epoch, 180 batch] loss: 2.60350, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:59:27.445987 Training: [34 epoch, 190 batch] loss: 2.43238, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:59:36.167638 Training: [34 epoch, 200 batch] loss: 2.45334, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:59:44.879563 Training: [34 epoch, 210 batch] loss: 2.51821, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 18:59:53.643595 Training: [34 epoch, 220 batch] loss: 2.46181, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:00:02.340486 Training: [34 epoch, 230 batch] loss: 2.45131, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:00:11.115689 Training: [34 epoch, 240 batch] loss: 2.42287, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:00:19.823052 Training: [34 epoch, 250 batch] loss: 2.49709, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:00:28.644915 Training: [34 epoch, 260 batch] loss: 2.46727, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:00:37.418584 Training: [34 epoch, 270 batch] loss: 2.43301, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:00:46.115054 Training: [34 epoch, 280 batch] loss: 2.44145, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:00:55.204721 Training: [34 epoch, 290 batch] loss: 2.43192, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:01:04.254565 Training: [34 epoch, 300 batch] loss: 2.42666, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:01:12.902720 Training: [34 epoch, 310 batch] loss: 2.46791, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:01:21.612163 Training: [34 epoch, 320 batch] loss: 2.43290, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:01:30.394019 Training: [34 epoch, 330 batch] loss: 2.45970, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:01:39.089463 Training: [34 epoch, 340 batch] loss: 2.45181, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:01:47.700682 Training: [34 epoch, 350 batch] loss: 2.43827, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:01:56.459993 Training: [34 epoch, 360 batch] loss: 2.45242, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:02:05.101953 Training: [34 epoch, 370 batch] loss: 2.48225, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:02:13.813355 Training: [34 epoch, 380 batch] loss: 2.44481, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "<Test> RMSE: 0.50341, MAE: 0.43194 \n",
            "2022-08-03 19:03:01.313210 Training: [35 epoch,  10 batch] loss: 2.44810, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:03:09.890330 Training: [35 epoch,  20 batch] loss: 2.49213, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:03:18.521611 Training: [35 epoch,  30 batch] loss: 2.45789, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:03:27.173243 Training: [35 epoch,  40 batch] loss: 2.41472, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:03:35.808974 Training: [35 epoch,  50 batch] loss: 2.47276, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:03:44.448939 Training: [35 epoch,  60 batch] loss: 2.45637, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:03:53.045453 Training: [35 epoch,  70 batch] loss: 2.41298, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:04:01.669561 Training: [35 epoch,  80 batch] loss: 2.47021, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:04:10.323344 Training: [35 epoch,  90 batch] loss: 2.45014, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:04:18.943167 Training: [35 epoch, 100 batch] loss: 2.47454, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:04:27.634547 Training: [35 epoch, 110 batch] loss: 2.44840, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:04:36.273181 Training: [35 epoch, 120 batch] loss: 2.50984, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:04:45.089769 Training: [35 epoch, 130 batch] loss: 2.46459, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:04:54.324017 Training: [35 epoch, 140 batch] loss: 2.44869, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:05:02.977149 Training: [35 epoch, 150 batch] loss: 2.43067, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:05:11.587724 Training: [35 epoch, 160 batch] loss: 2.59335, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:05:20.258464 Training: [35 epoch, 170 batch] loss: 2.42063, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:05:28.925686 Training: [35 epoch, 180 batch] loss: 2.43663, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:05:37.558929 Training: [35 epoch, 190 batch] loss: 2.46813, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:05:46.247600 Training: [35 epoch, 200 batch] loss: 2.42029, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:05:54.982940 Training: [35 epoch, 210 batch] loss: 2.45516, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:06:03.591056 Training: [35 epoch, 220 batch] loss: 2.42817, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:06:12.248573 Training: [35 epoch, 230 batch] loss: 2.40976, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:06:20.868011 Training: [35 epoch, 240 batch] loss: 2.47123, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:06:29.596071 Training: [35 epoch, 250 batch] loss: 2.43722, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:06:38.220600 Training: [35 epoch, 260 batch] loss: 2.45512, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:06:47.583100 Training: [35 epoch, 270 batch] loss: 2.44471, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:06:56.197547 Training: [35 epoch, 280 batch] loss: 2.50554, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:07:04.878939 Training: [35 epoch, 290 batch] loss: 2.43272, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:07:13.515579 Training: [35 epoch, 300 batch] loss: 2.47430, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:07:22.208260 Training: [35 epoch, 310 batch] loss: 2.48212, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:07:30.909708 Training: [35 epoch, 320 batch] loss: 2.42566, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:07:39.569839 Training: [35 epoch, 330 batch] loss: 2.49049, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:07:48.245264 Training: [35 epoch, 340 batch] loss: 2.45960, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:07:56.980625 Training: [35 epoch, 350 batch] loss: 2.44059, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:08:05.640993 Training: [35 epoch, 360 batch] loss: 2.43131, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:08:14.331270 Training: [35 epoch, 370 batch] loss: 2.41959, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:08:22.950739 Training: [35 epoch, 380 batch] loss: 2.44931, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "<Test> RMSE: 0.49595, MAE: 0.42166 \n",
            "2022-08-03 19:09:10.141896 Training: [36 epoch,  10 batch] loss: 2.41724, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:09:18.725192 Training: [36 epoch,  20 batch] loss: 2.49056, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:09:27.332037 Training: [36 epoch,  30 batch] loss: 2.43206, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:09:35.888485 Training: [36 epoch,  40 batch] loss: 2.44933, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:09:44.416131 Training: [36 epoch,  50 batch] loss: 2.44982, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:09:53.014978 Training: [36 epoch,  60 batch] loss: 2.46841, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:10:01.640388 Training: [36 epoch,  70 batch] loss: 2.43749, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:10:10.288317 Training: [36 epoch,  80 batch] loss: 2.43822, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:10:18.871167 Training: [36 epoch,  90 batch] loss: 2.42922, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:10:27.543531 Training: [36 epoch, 100 batch] loss: 2.43506, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:10:36.888293 Training: [36 epoch, 110 batch] loss: 2.45424, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:10:45.494043 Training: [36 epoch, 120 batch] loss: 2.54102, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:10:54.103251 Training: [36 epoch, 130 batch] loss: 2.42333, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:11:02.762622 Training: [36 epoch, 140 batch] loss: 2.42608, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:11:11.378401 Training: [36 epoch, 150 batch] loss: 2.46629, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:11:20.018031 Training: [36 epoch, 160 batch] loss: 2.49306, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:11:28.673982 Training: [36 epoch, 170 batch] loss: 2.42841, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:11:37.333691 Training: [36 epoch, 180 batch] loss: 2.61405, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:11:45.999951 Training: [36 epoch, 190 batch] loss: 2.44453, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:11:54.671539 Training: [36 epoch, 200 batch] loss: 2.48368, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:12:03.280135 Training: [36 epoch, 210 batch] loss: 2.46323, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:12:12.010696 Training: [36 epoch, 220 batch] loss: 2.44209, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:12:20.643693 Training: [36 epoch, 230 batch] loss: 2.45477, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:12:30.061358 Training: [36 epoch, 240 batch] loss: 2.48538, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:12:38.745758 Training: [36 epoch, 250 batch] loss: 2.49530, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:12:47.447189 Training: [36 epoch, 260 batch] loss: 2.43723, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:12:56.067755 Training: [36 epoch, 270 batch] loss: 2.42213, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:13:04.771003 Training: [36 epoch, 280 batch] loss: 2.43309, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:13:13.480046 Training: [36 epoch, 290 batch] loss: 2.41928, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:13:22.174328 Training: [36 epoch, 300 batch] loss: 2.53709, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:13:30.821205 Training: [36 epoch, 310 batch] loss: 2.43574, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:13:39.499332 Training: [36 epoch, 320 batch] loss: 2.44336, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:13:48.131942 Training: [36 epoch, 330 batch] loss: 2.47811, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:13:56.830488 Training: [36 epoch, 340 batch] loss: 2.40513, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:14:05.472234 Training: [36 epoch, 350 batch] loss: 2.44733, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:14:14.239466 Training: [36 epoch, 360 batch] loss: 2.44927, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:14:23.590143 Training: [36 epoch, 370 batch] loss: 2.43478, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:14:32.190486 Training: [36 epoch, 380 batch] loss: 2.46016, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "<Test> RMSE: 0.61424, MAE: 0.56696 \n",
            "2022-08-03 19:15:18.784884 Training: [37 epoch,  10 batch] loss: 2.42837, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:15:27.452697 Training: [37 epoch,  20 batch] loss: 2.43071, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:15:36.029660 Training: [37 epoch,  30 batch] loss: 2.43302, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:15:44.739042 Training: [37 epoch,  40 batch] loss: 2.42951, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:15:53.460385 Training: [37 epoch,  50 batch] loss: 2.42090, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:16:02.110565 Training: [37 epoch,  60 batch] loss: 2.41584, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:16:11.445181 Training: [37 epoch,  70 batch] loss: 2.59701, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:16:20.066031 Training: [37 epoch,  80 batch] loss: 2.49977, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:16:28.781252 Training: [37 epoch,  90 batch] loss: 2.44500, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:16:37.491928 Training: [37 epoch, 100 batch] loss: 2.45338, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:16:46.252114 Training: [37 epoch, 110 batch] loss: 2.44675, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:16:54.925352 Training: [37 epoch, 120 batch] loss: 2.41864, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:17:03.646429 Training: [37 epoch, 130 batch] loss: 2.42296, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:17:12.393424 Training: [37 epoch, 140 batch] loss: 2.44448, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:17:21.130354 Training: [37 epoch, 150 batch] loss: 2.46705, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:17:29.886956 Training: [37 epoch, 160 batch] loss: 2.44446, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:17:38.569239 Training: [37 epoch, 170 batch] loss: 2.43208, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:17:47.252407 Training: [37 epoch, 180 batch] loss: 2.42685, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:17:55.956772 Training: [37 epoch, 190 batch] loss: 2.45353, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:18:05.387176 Training: [37 epoch, 200 batch] loss: 2.44958, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:18:14.128258 Training: [37 epoch, 210 batch] loss: 2.44128, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:18:22.930526 Training: [37 epoch, 220 batch] loss: 2.49448, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:18:31.634981 Training: [37 epoch, 230 batch] loss: 2.47062, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:18:40.380233 Training: [37 epoch, 240 batch] loss: 2.45769, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:18:49.079817 Training: [37 epoch, 250 batch] loss: 2.44795, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:18:57.826533 Training: [37 epoch, 260 batch] loss: 2.43263, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:19:06.558472 Training: [37 epoch, 270 batch] loss: 2.52277, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:19:15.239884 Training: [37 epoch, 280 batch] loss: 2.43337, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:19:23.927801 Training: [37 epoch, 290 batch] loss: 2.48807, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:19:32.636062 Training: [37 epoch, 300 batch] loss: 2.51583, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:19:41.352660 Training: [37 epoch, 310 batch] loss: 2.42197, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:19:50.239623 Training: [37 epoch, 320 batch] loss: 2.47847, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:19:59.457777 Training: [37 epoch, 330 batch] loss: 2.44496, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:20:08.221435 Training: [37 epoch, 340 batch] loss: 2.46147, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:20:16.959852 Training: [37 epoch, 350 batch] loss: 2.46212, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:20:25.724543 Training: [37 epoch, 360 batch] loss: 2.44657, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:20:34.480278 Training: [37 epoch, 370 batch] loss: 2.44349, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 19:20:43.241928 Training: [37 epoch, 380 batch] loss: 2.44648, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "<Test> RMSE: 0.49815, MAE: 0.42472 \n",
            "The best RMSE/MAE: 0.38818 / 0.16549\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "a40NNTZVBd-M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}