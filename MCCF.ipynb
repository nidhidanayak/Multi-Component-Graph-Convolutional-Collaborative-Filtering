{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MCCF.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNnkw5dXPvU4RsjASjP1qcW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nidhidanayak/Multi-Component-Graph-Convolutional-Collaborative-Filtering/blob/master/MCCF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "1UYGVpAutTdX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "009b1050-b961-42b1-91cd-531deb57a139"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Github/\n",
        "git_token= \"iMfnKM1eP2ZRC1XQoQSecFVlngZfD52mCJXm\"  #your git token\n",
        "\n",
        "username= \"nidhidanayak\" #your username\n",
        "\n",
        "repository= \"Multi-Component-Graph-Convolutional-Collaborative-Filtering\""
      ],
      "metadata": {
        "id": "brVvqiT0AtC3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e22fcdb-bde2-48f6-ba1e-4f79aeb2cc2b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Github\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://{git_token}@github.com/{username}/{repository}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0WjjZAnIb-r",
        "outputId": "f38af8d3-ecf4-4fbc-9b3d-7af10a216f3f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Multi-Component-Graph-Convolutional-Collaborative-Filtering'...\n",
            "remote: Enumerating objects: 68, done.\u001b[K\n",
            "remote: Counting objects: 100% (68/68), done.\u001b[K\n",
            "remote: Compressing objects: 100% (55/55), done.\u001b[K\n",
            "remote: Total 68 (delta 23), reused 46 (delta 12), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (68/68), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd /content/drive/MyDrive/Github/Multi-Component-Graph-Convolutional-Collaborative-Filtering"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NctlVw5NBVzV",
        "outputId": "b1ffe7e6-01c2-40e6-ddee-fd262ed0b87a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Github/Multi-Component-Graph-Convolutional-Collaborative-Filtering\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python run.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DyPeRanwBYW4",
        "outputId": "7d63c14a-c891-4fa2-c973-2a718164500f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset: yelp\n",
            "-------------------- Hyperparams --------------------\n",
            "N: 30000\n",
            "weight decay: 0.0005\n",
            "dropout rate: 0.5\n",
            "learning rate: 0.001\n",
            "dimension of embedding: 64\n",
            "2022-08-03 15:33:37.330638 Training: [1 epoch,  10 batch] loss: 43.02707, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:33:45.863091 Training: [1 epoch,  20 batch] loss: 42.42303, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:33:54.332961 Training: [1 epoch,  30 batch] loss: 42.00899, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:34:02.858544 Training: [1 epoch,  40 batch] loss: 41.81264, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:34:11.761120 Training: [1 epoch,  50 batch] loss: 41.69831, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:34:20.844937 Training: [1 epoch,  60 batch] loss: 41.61350, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:34:29.423613 Training: [1 epoch,  70 batch] loss: 41.52994, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:34:37.982236 Training: [1 epoch,  80 batch] loss: 41.40965, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:34:46.623097 Training: [1 epoch,  90 batch] loss: 41.35451, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:34:55.324811 Training: [1 epoch, 100 batch] loss: 41.26853, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:35:03.966543 Training: [1 epoch, 110 batch] loss: 41.21390, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:35:12.553463 Training: [1 epoch, 120 batch] loss: 41.13419, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:35:21.248928 Training: [1 epoch, 130 batch] loss: 41.05774, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:35:29.925911 Training: [1 epoch, 140 batch] loss: 41.02324, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:35:38.577055 Training: [1 epoch, 150 batch] loss: 40.83817, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:35:47.276896 Training: [1 epoch, 160 batch] loss: 40.80947, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:35:55.973305 Training: [1 epoch, 170 batch] loss: 40.74362, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:36:04.597466 Training: [1 epoch, 180 batch] loss: 40.59872, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:36:13.262846 Training: [1 epoch, 190 batch] loss: 40.69973, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:36:21.921552 Training: [1 epoch, 200 batch] loss: 40.43122, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:36:30.626084 Training: [1 epoch, 210 batch] loss: 40.35577, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:36:40.166584 Training: [1 epoch, 220 batch] loss: 40.32778, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:36:48.817577 Training: [1 epoch, 230 batch] loss: 40.17775, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:36:57.470890 Training: [1 epoch, 240 batch] loss: 40.09875, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:37:06.114464 Training: [1 epoch, 250 batch] loss: 40.03945, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:37:14.746128 Training: [1 epoch, 260 batch] loss: 39.96331, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:37:23.499003 Training: [1 epoch, 270 batch] loss: 39.89438, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:37:32.155230 Training: [1 epoch, 280 batch] loss: 39.79430, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:37:40.852927 Training: [1 epoch, 290 batch] loss: 39.74479, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:37:49.545429 Training: [1 epoch, 300 batch] loss: 39.62606, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:37:58.196051 Training: [1 epoch, 310 batch] loss: 39.57766, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:38:06.832003 Training: [1 epoch, 320 batch] loss: 39.44452, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:38:15.489554 Training: [1 epoch, 330 batch] loss: 39.38849, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:38:24.115739 Training: [1 epoch, 340 batch] loss: 39.29746, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:38:32.781964 Training: [1 epoch, 350 batch] loss: 39.26631, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:38:41.429377 Training: [1 epoch, 360 batch] loss: 39.27843, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:38:50.126854 Training: [1 epoch, 370 batch] loss: 39.02477, the best RMSE/MAE: inf / inf\n",
            "2022-08-03 15:38:59.643974 Training: [1 epoch, 380 batch] loss: 38.95921, the best RMSE/MAE: inf / inf\n",
            "<Test> RMSE: 1487.40596, MAE: 1143.92163 \n",
            "2022-08-03 15:39:46.575091 Training: [2 epoch,  10 batch] loss: 38.75373, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:39:55.173250 Training: [2 epoch,  20 batch] loss: 38.80545, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:40:03.763127 Training: [2 epoch,  30 batch] loss: 38.53576, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:40:12.351896 Training: [2 epoch,  40 batch] loss: 38.45276, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:40:21.037544 Training: [2 epoch,  50 batch] loss: 38.37628, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:40:29.669090 Training: [2 epoch,  60 batch] loss: 38.25513, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:40:38.235972 Training: [2 epoch,  70 batch] loss: 38.18948, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:40:46.858445 Training: [2 epoch,  80 batch] loss: 38.15635, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:40:55.450281 Training: [2 epoch,  90 batch] loss: 37.95971, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:41:04.086616 Training: [2 epoch, 100 batch] loss: 37.99641, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:41:12.697450 Training: [2 epoch, 110 batch] loss: 37.73373, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:41:22.202454 Training: [2 epoch, 120 batch] loss: 37.65589, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:41:30.845718 Training: [2 epoch, 130 batch] loss: 37.54644, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:41:39.464037 Training: [2 epoch, 140 batch] loss: 37.48993, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:41:48.065460 Training: [2 epoch, 150 batch] loss: 37.47233, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:41:56.720459 Training: [2 epoch, 160 batch] loss: 37.24749, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:42:05.343301 Training: [2 epoch, 170 batch] loss: 37.12362, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:42:13.968571 Training: [2 epoch, 180 batch] loss: 37.05157, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:42:22.523594 Training: [2 epoch, 190 batch] loss: 36.96315, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:42:31.154841 Training: [2 epoch, 200 batch] loss: 36.80020, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:42:39.830430 Training: [2 epoch, 210 batch] loss: 36.66930, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:42:48.468080 Training: [2 epoch, 220 batch] loss: 36.57349, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:42:57.093751 Training: [2 epoch, 230 batch] loss: 36.49389, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:43:05.664175 Training: [2 epoch, 240 batch] loss: 36.37734, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:43:14.293652 Training: [2 epoch, 250 batch] loss: 36.26405, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:43:22.936577 Training: [2 epoch, 260 batch] loss: 36.16916, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:43:31.559129 Training: [2 epoch, 270 batch] loss: 36.01200, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:43:40.544677 Training: [2 epoch, 280 batch] loss: 35.90856, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:43:49.714216 Training: [2 epoch, 290 batch] loss: 35.77499, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:43:58.358502 Training: [2 epoch, 300 batch] loss: 35.65109, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:44:06.936990 Training: [2 epoch, 310 batch] loss: 35.54812, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:44:15.607505 Training: [2 epoch, 320 batch] loss: 35.43797, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:44:24.276934 Training: [2 epoch, 330 batch] loss: 35.30574, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:44:32.919248 Training: [2 epoch, 340 batch] loss: 35.19722, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:44:41.527390 Training: [2 epoch, 350 batch] loss: 35.06015, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:44:50.193831 Training: [2 epoch, 360 batch] loss: 34.99198, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:44:58.831596 Training: [2 epoch, 370 batch] loss: 34.88928, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "2022-08-03 15:45:07.498825 Training: [2 epoch, 380 batch] loss: 34.76005, the best RMSE/MAE: 1487.40596 / 1143.92163\n",
            "<Test> RMSE: 7.36587, MAE: 5.62144 \n",
            "2022-08-03 15:45:54.083839 Training: [3 epoch,  10 batch] loss: 34.51642, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:46:02.812600 Training: [3 epoch,  20 batch] loss: 34.38752, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:46:12.040018 Training: [3 epoch,  30 batch] loss: 34.23899, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:46:20.643716 Training: [3 epoch,  40 batch] loss: 34.23529, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:46:29.212817 Training: [3 epoch,  50 batch] loss: 34.03435, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:46:37.743344 Training: [3 epoch,  60 batch] loss: 33.88307, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:46:46.347063 Training: [3 epoch,  70 batch] loss: 33.79030, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:46:54.949886 Training: [3 epoch,  80 batch] loss: 33.63566, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:47:03.619805 Training: [3 epoch,  90 batch] loss: 33.51667, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:47:12.210174 Training: [3 epoch, 100 batch] loss: 33.35572, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:47:20.795371 Training: [3 epoch, 110 batch] loss: 33.27535, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:47:29.465782 Training: [3 epoch, 120 batch] loss: 33.21319, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:47:38.075074 Training: [3 epoch, 130 batch] loss: 33.02592, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:47:46.656043 Training: [3 epoch, 140 batch] loss: 32.83657, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:47:55.258743 Training: [3 epoch, 150 batch] loss: 32.81558, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:48:03.861097 Training: [3 epoch, 160 batch] loss: 32.60987, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:48:12.525321 Training: [3 epoch, 170 batch] loss: 32.44106, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:48:21.158611 Training: [3 epoch, 180 batch] loss: 32.36170, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:48:30.676939 Training: [3 epoch, 190 batch] loss: 32.20784, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:48:39.391249 Training: [3 epoch, 200 batch] loss: 32.04229, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:48:48.016055 Training: [3 epoch, 210 batch] loss: 32.13772, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:48:56.598898 Training: [3 epoch, 220 batch] loss: 31.77292, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:49:05.305785 Training: [3 epoch, 230 batch] loss: 31.67636, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:49:13.999005 Training: [3 epoch, 240 batch] loss: 31.52512, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:49:22.668559 Training: [3 epoch, 250 batch] loss: 31.41011, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:49:31.282200 Training: [3 epoch, 260 batch] loss: 31.22666, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:49:39.926842 Training: [3 epoch, 270 batch] loss: 31.12134, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:49:48.590441 Training: [3 epoch, 280 batch] loss: 31.00319, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:49:57.265839 Training: [3 epoch, 290 batch] loss: 30.85152, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:50:05.861600 Training: [3 epoch, 300 batch] loss: 30.65671, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:50:14.517396 Training: [3 epoch, 310 batch] loss: 30.62559, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:50:23.132027 Training: [3 epoch, 320 batch] loss: 30.43149, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:50:31.755671 Training: [3 epoch, 330 batch] loss: 30.25319, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:50:40.378594 Training: [3 epoch, 340 batch] loss: 30.16077, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:50:49.854814 Training: [3 epoch, 350 batch] loss: 29.97889, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:50:58.544064 Training: [3 epoch, 360 batch] loss: 29.92224, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:51:07.198240 Training: [3 epoch, 370 batch] loss: 29.73247, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "2022-08-03 15:51:15.856993 Training: [3 epoch, 380 batch] loss: 29.61423, the best RMSE/MAE: 7.36587 / 5.62144\n",
            "<Test> RMSE: 1.45382, MAE: 1.17113 \n",
            "2022-08-03 15:52:02.567627 Training: [4 epoch,  10 batch] loss: 29.33785, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:52:11.149640 Training: [4 epoch,  20 batch] loss: 29.24046, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:52:19.814206 Training: [4 epoch,  30 batch] loss: 29.11661, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:52:28.392059 Training: [4 epoch,  40 batch] loss: 29.10939, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:52:36.991916 Training: [4 epoch,  50 batch] loss: 28.83882, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:52:45.611193 Training: [4 epoch,  60 batch] loss: 28.63354, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:52:54.176546 Training: [4 epoch,  70 batch] loss: 28.59102, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:53:02.820638 Training: [4 epoch,  80 batch] loss: 28.44430, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:53:12.368238 Training: [4 epoch,  90 batch] loss: 28.19619, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:53:21.026319 Training: [4 epoch, 100 batch] loss: 28.06441, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:53:29.692265 Training: [4 epoch, 110 batch] loss: 27.92451, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:53:38.407932 Training: [4 epoch, 120 batch] loss: 27.82464, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:53:47.073587 Training: [4 epoch, 130 batch] loss: 27.68499, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:53:55.639542 Training: [4 epoch, 140 batch] loss: 27.48717, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:54:04.259233 Training: [4 epoch, 150 batch] loss: 27.38679, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:54:12.922996 Training: [4 epoch, 160 batch] loss: 27.22608, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:54:21.522316 Training: [4 epoch, 170 batch] loss: 27.09432, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:54:30.201369 Training: [4 epoch, 180 batch] loss: 26.91505, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:54:38.841639 Training: [4 epoch, 190 batch] loss: 26.79175, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:54:47.525286 Training: [4 epoch, 200 batch] loss: 26.67196, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:54:56.154963 Training: [4 epoch, 210 batch] loss: 26.56617, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:55:04.879487 Training: [4 epoch, 220 batch] loss: 26.37556, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:55:13.591426 Training: [4 epoch, 230 batch] loss: 26.30246, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:55:22.274293 Training: [4 epoch, 240 batch] loss: 26.14197, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:55:31.782272 Training: [4 epoch, 250 batch] loss: 25.93453, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:55:40.453696 Training: [4 epoch, 260 batch] loss: 25.87473, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:55:49.116335 Training: [4 epoch, 270 batch] loss: 25.66016, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:55:57.761869 Training: [4 epoch, 280 batch] loss: 25.54184, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:56:06.390871 Training: [4 epoch, 290 batch] loss: 25.41936, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:56:15.100146 Training: [4 epoch, 300 batch] loss: 25.23953, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:56:23.782378 Training: [4 epoch, 310 batch] loss: 25.13667, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:56:32.566899 Training: [4 epoch, 320 batch] loss: 24.95681, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:56:41.278952 Training: [4 epoch, 330 batch] loss: 24.83940, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:56:49.935533 Training: [4 epoch, 340 batch] loss: 24.74283, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:56:58.584910 Training: [4 epoch, 350 batch] loss: 24.53729, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:57:07.231260 Training: [4 epoch, 360 batch] loss: 24.39456, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:57:15.869782 Training: [4 epoch, 370 batch] loss: 24.28398, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "2022-08-03 15:57:24.488775 Training: [4 epoch, 380 batch] loss: 24.10797, the best RMSE/MAE: 1.45382 / 1.17113\n",
            "<Test> RMSE: 0.63661, MAE: 0.52145 \n",
            "2022-08-03 15:58:12.133849 Training: [5 epoch,  10 batch] loss: 23.89868, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 15:58:20.752029 Training: [5 epoch,  20 batch] loss: 23.78967, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 15:58:29.395844 Training: [5 epoch,  30 batch] loss: 23.64103, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 15:58:38.081572 Training: [5 epoch,  40 batch] loss: 23.61768, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 15:58:46.723964 Training: [5 epoch,  50 batch] loss: 23.40340, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 15:58:55.374891 Training: [5 epoch,  60 batch] loss: 23.19980, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 15:59:04.024337 Training: [5 epoch,  70 batch] loss: 23.08261, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 15:59:12.638599 Training: [5 epoch,  80 batch] loss: 23.10185, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 15:59:21.353902 Training: [5 epoch,  90 batch] loss: 22.83019, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 15:59:30.059361 Training: [5 epoch, 100 batch] loss: 22.70097, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 15:59:38.742063 Training: [5 epoch, 110 batch] loss: 22.59804, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 15:59:47.437732 Training: [5 epoch, 120 batch] loss: 22.40703, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 15:59:56.130008 Training: [5 epoch, 130 batch] loss: 22.30043, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 16:00:05.660301 Training: [5 epoch, 140 batch] loss: 22.12811, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 16:00:14.369367 Training: [5 epoch, 150 batch] loss: 22.08461, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 16:00:23.131354 Training: [5 epoch, 160 batch] loss: 21.86165, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 16:00:31.858124 Training: [5 epoch, 170 batch] loss: 21.73659, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 16:00:40.536230 Training: [5 epoch, 180 batch] loss: 21.62951, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 16:00:49.192348 Training: [5 epoch, 190 batch] loss: 21.48760, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 16:00:57.825760 Training: [5 epoch, 200 batch] loss: 21.45299, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 16:01:06.490476 Training: [5 epoch, 210 batch] loss: 21.24309, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 16:01:15.216948 Training: [5 epoch, 220 batch] loss: 21.12570, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 16:01:23.923175 Training: [5 epoch, 230 batch] loss: 20.97744, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 16:01:32.543114 Training: [5 epoch, 240 batch] loss: 20.85322, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 16:01:41.297492 Training: [5 epoch, 250 batch] loss: 20.73832, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 16:01:50.092420 Training: [5 epoch, 260 batch] loss: 20.59106, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 16:01:58.714355 Training: [5 epoch, 270 batch] loss: 20.47870, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 16:02:07.419810 Training: [5 epoch, 280 batch] loss: 20.39852, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 16:02:16.074722 Training: [5 epoch, 290 batch] loss: 20.22985, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 16:02:25.586889 Training: [5 epoch, 300 batch] loss: 20.07580, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 16:02:34.271525 Training: [5 epoch, 310 batch] loss: 19.99372, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 16:02:42.877120 Training: [5 epoch, 320 batch] loss: 19.85118, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 16:02:51.495842 Training: [5 epoch, 330 batch] loss: 19.75574, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 16:03:00.159929 Training: [5 epoch, 340 batch] loss: 19.63319, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 16:03:08.802207 Training: [5 epoch, 350 batch] loss: 19.50928, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 16:03:17.390026 Training: [5 epoch, 360 batch] loss: 19.36549, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 16:03:26.065368 Training: [5 epoch, 370 batch] loss: 19.27883, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "2022-08-03 16:03:34.743292 Training: [5 epoch, 380 batch] loss: 19.16750, the best RMSE/MAE: 0.63661 / 0.52145\n",
            "<Test> RMSE: 0.39494, MAE: 0.20803 \n",
            "2022-08-03 16:04:21.541076 Training: [6 epoch,  10 batch] loss: 18.94125, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:04:30.225445 Training: [6 epoch,  20 batch] loss: 18.80496, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:04:39.044492 Training: [6 epoch,  30 batch] loss: 18.66458, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:04:48.274917 Training: [6 epoch,  40 batch] loss: 18.61694, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:04:56.907182 Training: [6 epoch,  50 batch] loss: 18.44386, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:05:05.523075 Training: [6 epoch,  60 batch] loss: 18.34381, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:05:14.130228 Training: [6 epoch,  70 batch] loss: 18.25650, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:05:22.742338 Training: [6 epoch,  80 batch] loss: 18.18441, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:05:31.374065 Training: [6 epoch,  90 batch] loss: 18.00506, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:05:40.061683 Training: [6 epoch, 100 batch] loss: 17.89946, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:05:48.693915 Training: [6 epoch, 110 batch] loss: 17.77034, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:05:57.402607 Training: [6 epoch, 120 batch] loss: 17.72729, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:06:05.990792 Training: [6 epoch, 130 batch] loss: 17.60107, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:06:14.657648 Training: [6 epoch, 140 batch] loss: 17.48579, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:06:23.308686 Training: [6 epoch, 150 batch] loss: 17.30198, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:06:31.934840 Training: [6 epoch, 160 batch] loss: 17.22518, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:06:40.601676 Training: [6 epoch, 170 batch] loss: 17.08254, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:06:49.275786 Training: [6 epoch, 180 batch] loss: 17.00577, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:06:58.865508 Training: [6 epoch, 190 batch] loss: 16.86964, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:07:07.613570 Training: [6 epoch, 200 batch] loss: 16.77212, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:07:16.285698 Training: [6 epoch, 210 batch] loss: 16.89484, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:07:24.993801 Training: [6 epoch, 220 batch] loss: 16.56113, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:07:33.635598 Training: [6 epoch, 230 batch] loss: 16.43722, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:07:42.294432 Training: [6 epoch, 240 batch] loss: 16.37440, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:07:50.970282 Training: [6 epoch, 250 batch] loss: 16.25014, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:07:59.630972 Training: [6 epoch, 260 batch] loss: 16.26053, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:08:08.292491 Training: [6 epoch, 270 batch] loss: 16.06968, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:08:16.989761 Training: [6 epoch, 280 batch] loss: 15.96379, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:08:25.651527 Training: [6 epoch, 290 batch] loss: 15.87061, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:08:34.391819 Training: [6 epoch, 300 batch] loss: 15.84160, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:08:43.018107 Training: [6 epoch, 310 batch] loss: 15.68821, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:08:51.651334 Training: [6 epoch, 320 batch] loss: 15.51338, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:09:00.318227 Training: [6 epoch, 330 batch] loss: 15.45269, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:09:09.034339 Training: [6 epoch, 340 batch] loss: 15.39353, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:09:18.539501 Training: [6 epoch, 350 batch] loss: 15.25938, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:09:27.173179 Training: [6 epoch, 360 batch] loss: 15.19664, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:09:35.848894 Training: [6 epoch, 370 batch] loss: 15.10675, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "2022-08-03 16:09:44.525356 Training: [6 epoch, 380 batch] loss: 14.93576, the best RMSE/MAE: 0.39494 / 0.20803\n",
            "<Test> RMSE: 0.38818, MAE: 0.16549 \n",
            "2022-08-03 16:10:31.117213 Training: [7 epoch,  10 batch] loss: 14.84063, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:10:39.654540 Training: [7 epoch,  20 batch] loss: 14.73328, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:10:48.240816 Training: [7 epoch,  30 batch] loss: 14.66075, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:10:56.803257 Training: [7 epoch,  40 batch] loss: 14.52128, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:11:05.364719 Training: [7 epoch,  50 batch] loss: 14.44150, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:11:13.939536 Training: [7 epoch,  60 batch] loss: 14.35613, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:11:22.549161 Training: [7 epoch,  70 batch] loss: 14.25803, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:11:32.087249 Training: [7 epoch,  80 batch] loss: 14.17967, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:11:40.719089 Training: [7 epoch,  90 batch] loss: 14.10501, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:11:49.338254 Training: [7 epoch, 100 batch] loss: 13.98324, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:11:58.017293 Training: [7 epoch, 110 batch] loss: 13.87889, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:12:06.639502 Training: [7 epoch, 120 batch] loss: 13.82304, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:12:15.334271 Training: [7 epoch, 130 batch] loss: 13.73423, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:12:23.951618 Training: [7 epoch, 140 batch] loss: 13.63566, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:12:32.618293 Training: [7 epoch, 150 batch] loss: 13.57270, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:12:41.277739 Training: [7 epoch, 160 batch] loss: 13.43064, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:12:49.929325 Training: [7 epoch, 170 batch] loss: 13.37200, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:12:58.525340 Training: [7 epoch, 180 batch] loss: 13.31809, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:13:07.176004 Training: [7 epoch, 190 batch] loss: 13.16862, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:13:15.782358 Training: [7 epoch, 200 batch] loss: 13.20549, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:13:24.436860 Training: [7 epoch, 210 batch] loss: 13.07829, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:13:33.027957 Training: [7 epoch, 220 batch] loss: 12.96132, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:13:41.707725 Training: [7 epoch, 230 batch] loss: 12.86229, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:13:51.290372 Training: [7 epoch, 240 batch] loss: 12.77544, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:14:00.018023 Training: [7 epoch, 250 batch] loss: 12.69836, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:14:08.634131 Training: [7 epoch, 260 batch] loss: 12.70714, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:14:17.264148 Training: [7 epoch, 270 batch] loss: 12.57068, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:14:25.889772 Training: [7 epoch, 280 batch] loss: 12.48284, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:14:34.558302 Training: [7 epoch, 290 batch] loss: 12.46500, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:14:43.197766 Training: [7 epoch, 300 batch] loss: 12.30536, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:14:51.876389 Training: [7 epoch, 310 batch] loss: 12.23095, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:15:00.511893 Training: [7 epoch, 320 batch] loss: 12.17689, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:15:09.187015 Training: [7 epoch, 330 batch] loss: 12.08710, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:15:17.817417 Training: [7 epoch, 340 batch] loss: 12.03888, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:15:26.451654 Training: [7 epoch, 350 batch] loss: 11.94948, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:15:35.051854 Training: [7 epoch, 360 batch] loss: 11.96967, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:15:43.729428 Training: [7 epoch, 370 batch] loss: 11.80189, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:15:52.312984 Training: [7 epoch, 380 batch] loss: 11.91290, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "<Test> RMSE: 0.38851, MAE: 0.15471 \n",
            "2022-08-03 16:16:39.765228 Training: [8 epoch,  10 batch] loss: 11.55808, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:16:48.384458 Training: [8 epoch,  20 batch] loss: 11.54023, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:16:56.947320 Training: [8 epoch,  30 batch] loss: 11.43882, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:17:05.540901 Training: [8 epoch,  40 batch] loss: 11.38105, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:17:14.188106 Training: [8 epoch,  50 batch] loss: 11.29477, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:17:22.857830 Training: [8 epoch,  60 batch] loss: 11.26226, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:17:31.473196 Training: [8 epoch,  70 batch] loss: 11.21238, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:17:40.082403 Training: [8 epoch,  80 batch] loss: 11.11056, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:17:48.679254 Training: [8 epoch,  90 batch] loss: 11.09296, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:17:57.327552 Training: [8 epoch, 100 batch] loss: 10.98168, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:18:06.004245 Training: [8 epoch, 110 batch] loss: 10.89658, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:18:14.644409 Training: [8 epoch, 120 batch] loss: 10.84190, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:18:24.116892 Training: [8 epoch, 130 batch] loss: 10.77703, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:18:32.759030 Training: [8 epoch, 140 batch] loss: 10.73726, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:18:41.402651 Training: [8 epoch, 150 batch] loss: 10.64616, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:18:50.044666 Training: [8 epoch, 160 batch] loss: 10.59368, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:18:58.684401 Training: [8 epoch, 170 batch] loss: 10.55371, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:19:07.336003 Training: [8 epoch, 180 batch] loss: 10.39386, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:19:16.095291 Training: [8 epoch, 190 batch] loss: 10.41013, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:19:24.731071 Training: [8 epoch, 200 batch] loss: 10.35986, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:19:33.335229 Training: [8 epoch, 210 batch] loss: 10.20007, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:19:41.938989 Training: [8 epoch, 220 batch] loss: 10.16612, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:19:50.622100 Training: [8 epoch, 230 batch] loss: 10.15740, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:19:59.296428 Training: [8 epoch, 240 batch] loss: 10.05308, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:20:07.953091 Training: [8 epoch, 250 batch] loss: 10.02056, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:20:16.559005 Training: [8 epoch, 260 batch] loss: 9.94691, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:20:25.232403 Training: [8 epoch, 270 batch] loss: 9.89427, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:20:33.923918 Training: [8 epoch, 280 batch] loss: 9.81638, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:20:43.488308 Training: [8 epoch, 290 batch] loss: 9.73423, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:20:52.128326 Training: [8 epoch, 300 batch] loss: 9.67472, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:21:00.755650 Training: [8 epoch, 310 batch] loss: 9.69097, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:21:09.393858 Training: [8 epoch, 320 batch] loss: 9.60534, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:21:18.066428 Training: [8 epoch, 330 batch] loss: 9.67841, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:21:26.711054 Training: [8 epoch, 340 batch] loss: 9.53977, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:21:35.379246 Training: [8 epoch, 350 batch] loss: 9.42746, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:21:44.072831 Training: [8 epoch, 360 batch] loss: 9.37710, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:21:52.766514 Training: [8 epoch, 370 batch] loss: 9.34826, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:22:01.526685 Training: [8 epoch, 380 batch] loss: 9.24387, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "<Test> RMSE: 0.39366, MAE: 0.19986 \n",
            "2022-08-03 16:22:48.514007 Training: [9 epoch,  10 batch] loss: 9.13558, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:22:58.006665 Training: [9 epoch,  20 batch] loss: 9.11297, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:23:06.661440 Training: [9 epoch,  30 batch] loss: 9.13261, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:23:15.278259 Training: [9 epoch,  40 batch] loss: 9.00863, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:23:23.879182 Training: [9 epoch,  50 batch] loss: 8.90407, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:23:32.508354 Training: [9 epoch,  60 batch] loss: 8.84265, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:23:41.193654 Training: [9 epoch,  70 batch] loss: 8.82922, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:23:49.876517 Training: [9 epoch,  80 batch] loss: 8.76393, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:23:58.541284 Training: [9 epoch,  90 batch] loss: 8.71865, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:24:07.247473 Training: [9 epoch, 100 batch] loss: 8.63020, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:24:15.947962 Training: [9 epoch, 110 batch] loss: 8.58587, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:24:24.719621 Training: [9 epoch, 120 batch] loss: 8.63571, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:24:33.399342 Training: [9 epoch, 130 batch] loss: 8.50017, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:24:42.134728 Training: [9 epoch, 140 batch] loss: 8.45415, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:24:50.808183 Training: [9 epoch, 150 batch] loss: 8.38385, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:24:59.514756 Training: [9 epoch, 160 batch] loss: 8.38501, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:25:08.166584 Training: [9 epoch, 170 batch] loss: 8.35104, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:25:17.712668 Training: [9 epoch, 180 batch] loss: 8.25192, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:25:26.391992 Training: [9 epoch, 190 batch] loss: 8.23824, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:25:35.027137 Training: [9 epoch, 200 batch] loss: 8.44705, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:25:43.694831 Training: [9 epoch, 210 batch] loss: 8.15661, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:25:52.334138 Training: [9 epoch, 220 batch] loss: 8.14117, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:26:01.067888 Training: [9 epoch, 230 batch] loss: 8.08342, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:26:09.691341 Training: [9 epoch, 240 batch] loss: 7.95579, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:26:18.351707 Training: [9 epoch, 250 batch] loss: 7.93835, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:26:27.082611 Training: [9 epoch, 260 batch] loss: 7.85835, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:26:35.766558 Training: [9 epoch, 270 batch] loss: 7.86444, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:26:44.418637 Training: [9 epoch, 280 batch] loss: 7.78422, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:26:53.088397 Training: [9 epoch, 290 batch] loss: 7.70759, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:27:01.794726 Training: [9 epoch, 300 batch] loss: 7.69817, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:27:10.481409 Training: [9 epoch, 310 batch] loss: 7.68879, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:27:19.215966 Training: [9 epoch, 320 batch] loss: 7.60760, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:27:27.899773 Training: [9 epoch, 330 batch] loss: 7.66640, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:27:37.523429 Training: [9 epoch, 340 batch] loss: 7.53818, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:27:46.242932 Training: [9 epoch, 350 batch] loss: 7.47298, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:27:54.887433 Training: [9 epoch, 360 batch] loss: 7.44449, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:28:03.541539 Training: [9 epoch, 370 batch] loss: 7.45856, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:28:12.232664 Training: [9 epoch, 380 batch] loss: 7.36874, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "<Test> RMSE: 0.39445, MAE: 0.21610 \n",
            "2022-08-03 16:28:59.121017 Training: [10 epoch,  10 batch] loss: 7.28832, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:29:07.705476 Training: [10 epoch,  20 batch] loss: 7.33772, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:29:16.351399 Training: [10 epoch,  30 batch] loss: 7.20450, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:29:24.946609 Training: [10 epoch,  40 batch] loss: 7.22760, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:29:33.609852 Training: [10 epoch,  50 batch] loss: 7.09339, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:29:42.310585 Training: [10 epoch,  60 batch] loss: 7.05625, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:29:51.784564 Training: [10 epoch,  70 batch] loss: 7.08530, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:30:00.475841 Training: [10 epoch,  80 batch] loss: 7.02785, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:30:09.163674 Training: [10 epoch,  90 batch] loss: 7.03318, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:30:17.800176 Training: [10 epoch, 100 batch] loss: 6.89974, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:30:26.470422 Training: [10 epoch, 110 batch] loss: 7.04166, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:30:35.106514 Training: [10 epoch, 120 batch] loss: 6.84128, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:30:43.814606 Training: [10 epoch, 130 batch] loss: 6.79242, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:30:52.471960 Training: [10 epoch, 140 batch] loss: 6.76178, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:31:01.157902 Training: [10 epoch, 150 batch] loss: 6.72318, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:31:09.765480 Training: [10 epoch, 160 batch] loss: 6.68254, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:31:18.492057 Training: [10 epoch, 170 batch] loss: 6.63675, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:31:27.191737 Training: [10 epoch, 180 batch] loss: 6.63696, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:31:35.883359 Training: [10 epoch, 190 batch] loss: 6.59606, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:31:44.472031 Training: [10 epoch, 200 batch] loss: 6.57932, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:31:53.138910 Training: [10 epoch, 210 batch] loss: 6.48812, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:32:01.836588 Training: [10 epoch, 220 batch] loss: 6.45759, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:32:11.394509 Training: [10 epoch, 230 batch] loss: 6.41307, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:32:20.046139 Training: [10 epoch, 240 batch] loss: 6.40544, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:32:28.759174 Training: [10 epoch, 250 batch] loss: 6.34509, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:32:37.512075 Training: [10 epoch, 260 batch] loss: 6.32817, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:32:46.287798 Training: [10 epoch, 270 batch] loss: 6.25086, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:32:54.986075 Training: [10 epoch, 280 batch] loss: 6.30447, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:33:03.652898 Training: [10 epoch, 290 batch] loss: 6.26822, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:33:12.299271 Training: [10 epoch, 300 batch] loss: 6.19764, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:33:20.941737 Training: [10 epoch, 310 batch] loss: 6.16028, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:33:29.605397 Training: [10 epoch, 320 batch] loss: 6.15999, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:33:38.285814 Training: [10 epoch, 330 batch] loss: 6.10216, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:33:46.937082 Training: [10 epoch, 340 batch] loss: 6.06202, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:33:55.636454 Training: [10 epoch, 350 batch] loss: 6.07884, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:34:04.354246 Training: [10 epoch, 360 batch] loss: 6.01971, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:34:13.020150 Training: [10 epoch, 370 batch] loss: 5.93677, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:34:22.518402 Training: [10 epoch, 380 batch] loss: 5.92853, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "<Test> RMSE: 0.40321, MAE: 0.24876 \n",
            "2022-08-03 16:35:09.140991 Training: [11 epoch,  10 batch] loss: 5.86511, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:35:17.740944 Training: [11 epoch,  20 batch] loss: 5.88853, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:35:26.362156 Training: [11 epoch,  30 batch] loss: 5.77244, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:35:35.002397 Training: [11 epoch,  40 batch] loss: 5.78862, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:35:43.598461 Training: [11 epoch,  50 batch] loss: 5.71747, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:35:52.148994 Training: [11 epoch,  60 batch] loss: 5.72868, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:36:00.738215 Training: [11 epoch,  70 batch] loss: 5.66120, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:36:09.336446 Training: [11 epoch,  80 batch] loss: 5.63958, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:36:17.960917 Training: [11 epoch,  90 batch] loss: 5.77577, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:36:26.620129 Training: [11 epoch, 100 batch] loss: 5.63078, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:36:35.789182 Training: [11 epoch, 110 batch] loss: 5.54249, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:36:44.771719 Training: [11 epoch, 120 batch] loss: 5.58782, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:36:53.506554 Training: [11 epoch, 130 batch] loss: 5.52214, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:37:02.236626 Training: [11 epoch, 140 batch] loss: 5.50745, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:37:10.953790 Training: [11 epoch, 150 batch] loss: 5.52668, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:37:19.610994 Training: [11 epoch, 160 batch] loss: 5.39417, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:37:28.296244 Training: [11 epoch, 170 batch] loss: 5.37706, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:37:37.012084 Training: [11 epoch, 180 batch] loss: 5.36461, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:37:45.653417 Training: [11 epoch, 190 batch] loss: 5.40190, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:37:54.394265 Training: [11 epoch, 200 batch] loss: 5.27604, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:38:03.154295 Training: [11 epoch, 210 batch] loss: 5.27491, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:38:11.898680 Training: [11 epoch, 220 batch] loss: 5.22390, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:38:20.525872 Training: [11 epoch, 230 batch] loss: 5.26812, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:38:29.270687 Training: [11 epoch, 240 batch] loss: 5.15845, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:38:37.967246 Training: [11 epoch, 250 batch] loss: 5.18009, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:38:46.602222 Training: [11 epoch, 260 batch] loss: 5.17297, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:38:56.080059 Training: [11 epoch, 270 batch] loss: 5.10567, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:39:04.764529 Training: [11 epoch, 280 batch] loss: 5.06438, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:39:13.461732 Training: [11 epoch, 290 batch] loss: 5.04705, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:39:22.151340 Training: [11 epoch, 300 batch] loss: 5.00399, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:39:30.823789 Training: [11 epoch, 310 batch] loss: 4.99061, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:39:39.512934 Training: [11 epoch, 320 batch] loss: 4.99835, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:39:48.143233 Training: [11 epoch, 330 batch] loss: 4.92015, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:39:56.854548 Training: [11 epoch, 340 batch] loss: 4.89887, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:40:05.457431 Training: [11 epoch, 350 batch] loss: 4.89928, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:40:14.122135 Training: [11 epoch, 360 batch] loss: 4.82947, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:40:22.789837 Training: [11 epoch, 370 batch] loss: 4.81724, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:40:31.474061 Training: [11 epoch, 380 batch] loss: 4.78070, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "<Test> RMSE: 0.40062, MAE: 0.24000 \n",
            "2022-08-03 16:41:19.316439 Training: [12 epoch,  10 batch] loss: 4.76884, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:41:28.160097 Training: [12 epoch,  20 batch] loss: 4.77053, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:41:36.965397 Training: [12 epoch,  30 batch] loss: 4.77741, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:41:45.817701 Training: [12 epoch,  40 batch] loss: 4.67486, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:41:54.412931 Training: [12 epoch,  50 batch] loss: 4.63793, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:42:03.019575 Training: [12 epoch,  60 batch] loss: 4.70033, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:42:11.657808 Training: [12 epoch,  70 batch] loss: 4.67445, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:42:20.305516 Training: [12 epoch,  80 batch] loss: 4.58718, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:42:28.981555 Training: [12 epoch,  90 batch] loss: 4.57378, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:42:37.627140 Training: [12 epoch, 100 batch] loss: 4.54099, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:42:46.258744 Training: [12 epoch, 110 batch] loss: 4.50735, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:42:54.865529 Training: [12 epoch, 120 batch] loss: 4.67511, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:43:03.593937 Training: [12 epoch, 130 batch] loss: 4.47547, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:43:12.286049 Training: [12 epoch, 140 batch] loss: 4.45773, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:43:20.965010 Training: [12 epoch, 150 batch] loss: 4.50839, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:43:30.493920 Training: [12 epoch, 160 batch] loss: 4.39094, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:43:39.159814 Training: [12 epoch, 170 batch] loss: 4.36020, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:43:47.824325 Training: [12 epoch, 180 batch] loss: 4.33687, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:43:56.424898 Training: [12 epoch, 190 batch] loss: 4.32924, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:44:05.113614 Training: [12 epoch, 200 batch] loss: 4.33838, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:44:13.780757 Training: [12 epoch, 210 batch] loss: 4.28617, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:44:22.428888 Training: [12 epoch, 220 batch] loss: 4.24777, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:44:31.155528 Training: [12 epoch, 230 batch] loss: 4.35326, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:44:39.858410 Training: [12 epoch, 240 batch] loss: 4.29595, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:44:48.601876 Training: [12 epoch, 250 batch] loss: 4.20610, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:44:57.224960 Training: [12 epoch, 260 batch] loss: 4.18741, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:45:05.920064 Training: [12 epoch, 270 batch] loss: 4.19277, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:45:14.652396 Training: [12 epoch, 280 batch] loss: 4.13967, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:45:23.378596 Training: [12 epoch, 290 batch] loss: 4.17846, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:45:32.021950 Training: [12 epoch, 300 batch] loss: 4.14274, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:45:41.552822 Training: [12 epoch, 310 batch] loss: 4.10456, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:45:50.187249 Training: [12 epoch, 320 batch] loss: 4.11844, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:45:58.901071 Training: [12 epoch, 330 batch] loss: 4.06272, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:46:07.543097 Training: [12 epoch, 340 batch] loss: 3.98053, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:46:16.276834 Training: [12 epoch, 350 batch] loss: 3.98323, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:46:24.958541 Training: [12 epoch, 360 batch] loss: 3.95467, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:46:33.655643 Training: [12 epoch, 370 batch] loss: 3.97910, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:46:42.288036 Training: [12 epoch, 380 batch] loss: 3.93963, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "<Test> RMSE: 0.44829, MAE: 0.34836 \n",
            "2022-08-03 16:47:28.763159 Training: [13 epoch,  10 batch] loss: 3.89243, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:47:37.413312 Training: [13 epoch,  20 batch] loss: 3.92400, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:47:46.037610 Training: [13 epoch,  30 batch] loss: 4.04085, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:47:55.331210 Training: [13 epoch,  40 batch] loss: 3.93034, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:48:04.035738 Training: [13 epoch,  50 batch] loss: 3.81463, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:48:12.674376 Training: [13 epoch,  60 batch] loss: 3.81127, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:48:21.346628 Training: [13 epoch,  70 batch] loss: 3.78197, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:48:29.988625 Training: [13 epoch,  80 batch] loss: 3.84528, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:48:38.597909 Training: [13 epoch,  90 batch] loss: 3.71777, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:48:47.287273 Training: [13 epoch, 100 batch] loss: 3.72106, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:48:55.976076 Training: [13 epoch, 110 batch] loss: 3.72744, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:49:04.679130 Training: [13 epoch, 120 batch] loss: 3.69724, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:49:13.359388 Training: [13 epoch, 130 batch] loss: 3.67265, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:49:22.009827 Training: [13 epoch, 140 batch] loss: 3.65130, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:49:30.710580 Training: [13 epoch, 150 batch] loss: 3.71165, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:49:39.376282 Training: [13 epoch, 160 batch] loss: 3.62823, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:49:48.044994 Training: [13 epoch, 170 batch] loss: 3.60004, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:49:56.726660 Training: [13 epoch, 180 batch] loss: 3.58782, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:50:05.451951 Training: [13 epoch, 190 batch] loss: 3.60219, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:50:15.060607 Training: [13 epoch, 200 batch] loss: 3.54119, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:50:23.700040 Training: [13 epoch, 210 batch] loss: 3.58997, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:50:32.461809 Training: [13 epoch, 220 batch] loss: 3.49615, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:50:41.137230 Training: [13 epoch, 230 batch] loss: 3.46637, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:50:49.794780 Training: [13 epoch, 240 batch] loss: 3.49551, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:50:58.425605 Training: [13 epoch, 250 batch] loss: 3.45339, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:51:07.126744 Training: [13 epoch, 260 batch] loss: 3.45242, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:51:15.796573 Training: [13 epoch, 270 batch] loss: 3.40716, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:51:24.451983 Training: [13 epoch, 280 batch] loss: 3.39836, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:51:33.154506 Training: [13 epoch, 290 batch] loss: 3.46926, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:51:41.885493 Training: [13 epoch, 300 batch] loss: 3.39544, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:51:50.543991 Training: [13 epoch, 310 batch] loss: 3.40715, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:51:59.199000 Training: [13 epoch, 320 batch] loss: 3.43750, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:52:07.849394 Training: [13 epoch, 330 batch] loss: 3.39513, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:52:16.537728 Training: [13 epoch, 340 batch] loss: 3.31968, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:52:25.757403 Training: [13 epoch, 350 batch] loss: 3.36403, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:52:34.803126 Training: [13 epoch, 360 batch] loss: 3.26690, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:52:43.486950 Training: [13 epoch, 370 batch] loss: 3.28172, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:52:52.182207 Training: [13 epoch, 380 batch] loss: 3.32229, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "<Test> RMSE: 0.40823, MAE: 0.26358 \n",
            "2022-08-03 16:53:38.828429 Training: [14 epoch,  10 batch] loss: 3.24210, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:53:47.437894 Training: [14 epoch,  20 batch] loss: 3.27776, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:53:56.049097 Training: [14 epoch,  30 batch] loss: 3.21750, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:54:04.659723 Training: [14 epoch,  40 batch] loss: 3.22322, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:54:13.363453 Training: [14 epoch,  50 batch] loss: 3.17403, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:54:22.009281 Training: [14 epoch,  60 batch] loss: 3.14373, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:54:30.688052 Training: [14 epoch,  70 batch] loss: 3.12202, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:54:39.322048 Training: [14 epoch,  80 batch] loss: 3.13182, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:54:48.847231 Training: [14 epoch,  90 batch] loss: 3.13372, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:54:57.494332 Training: [14 epoch, 100 batch] loss: 3.07674, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:55:06.177834 Training: [14 epoch, 110 batch] loss: 3.07386, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:55:14.804319 Training: [14 epoch, 120 batch] loss: 3.08354, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:55:23.468287 Training: [14 epoch, 130 batch] loss: 3.05027, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:55:32.201327 Training: [14 epoch, 140 batch] loss: 3.03819, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:55:40.892683 Training: [14 epoch, 150 batch] loss: 3.01526, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:55:49.572065 Training: [14 epoch, 160 batch] loss: 3.15213, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:55:58.225922 Training: [14 epoch, 170 batch] loss: 3.04190, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:56:06.887714 Training: [14 epoch, 180 batch] loss: 3.02141, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:56:15.593665 Training: [14 epoch, 190 batch] loss: 2.94665, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:56:24.285174 Training: [14 epoch, 200 batch] loss: 2.93305, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:56:33.037304 Training: [14 epoch, 210 batch] loss: 2.98988, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:56:41.728855 Training: [14 epoch, 220 batch] loss: 2.91491, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:56:50.374461 Training: [14 epoch, 230 batch] loss: 2.92382, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:56:59.914982 Training: [14 epoch, 240 batch] loss: 2.94191, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:57:08.616783 Training: [14 epoch, 250 batch] loss: 2.87844, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:57:17.317462 Training: [14 epoch, 260 batch] loss: 2.88082, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:57:26.039012 Training: [14 epoch, 270 batch] loss: 2.85378, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:57:34.705230 Training: [14 epoch, 280 batch] loss: 2.82850, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:57:43.394314 Training: [14 epoch, 290 batch] loss: 2.89645, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:57:52.085707 Training: [14 epoch, 300 batch] loss: 2.81591, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:58:00.737388 Training: [14 epoch, 310 batch] loss: 2.84271, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:58:09.453664 Training: [14 epoch, 320 batch] loss: 2.81859, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:58:18.086386 Training: [14 epoch, 330 batch] loss: 2.86383, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:58:26.716724 Training: [14 epoch, 340 batch] loss: 2.72703, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:58:35.462488 Training: [14 epoch, 350 batch] loss: 2.75933, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:58:44.187395 Training: [14 epoch, 360 batch] loss: 2.72083, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:58:52.810747 Training: [14 epoch, 370 batch] loss: 2.71724, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:59:01.545327 Training: [14 epoch, 380 batch] loss: 2.69705, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "<Test> RMSE: 0.41102, MAE: 0.27116 \n",
            "2022-08-03 16:59:49.227914 Training: [15 epoch,  10 batch] loss: 2.67779, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 16:59:57.834853 Training: [15 epoch,  20 batch] loss: 2.67556, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:00:06.362818 Training: [15 epoch,  30 batch] loss: 2.64094, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:00:15.021108 Training: [15 epoch,  40 batch] loss: 2.69877, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:00:23.681671 Training: [15 epoch,  50 batch] loss: 2.63020, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:00:32.391463 Training: [15 epoch,  60 batch] loss: 2.64213, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:00:41.067188 Training: [15 epoch,  70 batch] loss: 2.60040, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:00:49.750750 Training: [15 epoch,  80 batch] loss: 2.60480, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:00:58.430095 Training: [15 epoch,  90 batch] loss: 2.57909, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:01:07.106028 Training: [15 epoch, 100 batch] loss: 2.57264, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:01:15.759600 Training: [15 epoch, 110 batch] loss: 2.59241, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:01:24.384215 Training: [15 epoch, 120 batch] loss: 2.62750, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:01:33.901445 Training: [15 epoch, 130 batch] loss: 2.51970, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:01:42.607462 Training: [15 epoch, 140 batch] loss: 2.52274, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:01:51.234870 Training: [15 epoch, 150 batch] loss: 2.50095, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:01:59.890453 Training: [15 epoch, 160 batch] loss: 2.50276, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:02:08.585503 Training: [15 epoch, 170 batch] loss: 2.50726, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:02:17.330273 Training: [15 epoch, 180 batch] loss: 2.51407, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:02:26.040934 Training: [15 epoch, 190 batch] loss: 2.56885, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:02:34.695014 Training: [15 epoch, 200 batch] loss: 2.47297, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:02:43.367670 Training: [15 epoch, 210 batch] loss: 2.71953, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:02:52.034329 Training: [15 epoch, 220 batch] loss: 2.47265, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:03:00.697127 Training: [15 epoch, 230 batch] loss: 2.48272, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:03:09.325881 Training: [15 epoch, 240 batch] loss: 2.48681, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:03:17.992596 Training: [15 epoch, 250 batch] loss: 2.56575, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:03:26.625533 Training: [15 epoch, 260 batch] loss: 2.48687, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:03:35.336284 Training: [15 epoch, 270 batch] loss: 2.54023, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:03:44.870043 Training: [15 epoch, 280 batch] loss: 2.51812, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:03:53.590715 Training: [15 epoch, 290 batch] loss: 2.49368, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:04:02.240859 Training: [15 epoch, 300 batch] loss: 2.47973, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:04:10.909408 Training: [15 epoch, 310 batch] loss: 2.48993, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:04:19.543385 Training: [15 epoch, 320 batch] loss: 2.47247, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:04:28.253758 Training: [15 epoch, 330 batch] loss: 2.48390, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:04:36.885736 Training: [15 epoch, 340 batch] loss: 2.45610, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:04:45.550188 Training: [15 epoch, 350 batch] loss: 2.46997, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:04:54.205186 Training: [15 epoch, 360 batch] loss: 2.53764, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:05:02.913668 Training: [15 epoch, 370 batch] loss: 2.58214, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:05:11.605805 Training: [15 epoch, 380 batch] loss: 2.48610, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "<Test> RMSE: 0.40132, MAE: 0.24248 \n",
            "2022-08-03 17:05:58.731653 Training: [16 epoch,  10 batch] loss: 2.54444, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:06:07.535569 Training: [16 epoch,  20 batch] loss: 2.44844, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:06:16.104540 Training: [16 epoch,  30 batch] loss: 2.45579, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:06:24.723605 Training: [16 epoch,  40 batch] loss: 2.45556, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:06:33.300198 Training: [16 epoch,  50 batch] loss: 2.47975, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:06:41.895713 Training: [16 epoch,  60 batch] loss: 2.45144, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:06:50.478583 Training: [16 epoch,  70 batch] loss: 2.49764, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:06:59.104757 Training: [16 epoch,  80 batch] loss: 2.45213, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:07:07.723023 Training: [16 epoch,  90 batch] loss: 2.46030, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:07:16.323164 Training: [16 epoch, 100 batch] loss: 2.62926, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:07:24.949776 Training: [16 epoch, 110 batch] loss: 2.48430, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:07:33.576469 Training: [16 epoch, 120 batch] loss: 2.43954, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:07:42.253176 Training: [16 epoch, 130 batch] loss: 2.51214, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:07:50.899664 Training: [16 epoch, 140 batch] loss: 2.46121, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:07:59.506947 Training: [16 epoch, 150 batch] loss: 2.47190, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:08:08.108522 Training: [16 epoch, 160 batch] loss: 2.47603, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:08:17.621172 Training: [16 epoch, 170 batch] loss: 2.45042, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:08:26.315350 Training: [16 epoch, 180 batch] loss: 2.44764, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:08:34.961547 Training: [16 epoch, 190 batch] loss: 2.49287, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:08:43.673113 Training: [16 epoch, 200 batch] loss: 2.45057, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:08:52.401815 Training: [16 epoch, 210 batch] loss: 2.51627, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:09:01.160858 Training: [16 epoch, 220 batch] loss: 2.47774, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:09:09.880831 Training: [16 epoch, 230 batch] loss: 2.47234, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:09:18.521747 Training: [16 epoch, 240 batch] loss: 2.47256, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:09:27.194124 Training: [16 epoch, 250 batch] loss: 2.51151, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:09:35.858685 Training: [16 epoch, 260 batch] loss: 2.44903, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:09:44.499560 Training: [16 epoch, 270 batch] loss: 2.45180, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:09:53.191124 Training: [16 epoch, 280 batch] loss: 2.44765, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:10:01.855862 Training: [16 epoch, 290 batch] loss: 2.45668, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:10:10.588279 Training: [16 epoch, 300 batch] loss: 2.47230, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:10:19.252955 Training: [16 epoch, 310 batch] loss: 2.44591, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:10:28.803817 Training: [16 epoch, 320 batch] loss: 2.44075, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:10:37.490197 Training: [16 epoch, 330 batch] loss: 2.52329, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:10:46.181657 Training: [16 epoch, 340 batch] loss: 2.51249, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:10:54.842287 Training: [16 epoch, 350 batch] loss: 2.47901, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:11:03.493237 Training: [16 epoch, 360 batch] loss: 2.48341, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:11:12.175305 Training: [16 epoch, 370 batch] loss: 2.43977, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:11:20.863972 Training: [16 epoch, 380 batch] loss: 2.49813, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "<Test> RMSE: 0.43033, MAE: 0.31504 \n",
            "2022-08-03 17:12:07.829433 Training: [17 epoch,  10 batch] loss: 2.47452, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:12:16.387553 Training: [17 epoch,  20 batch] loss: 2.47030, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:12:24.987464 Training: [17 epoch,  30 batch] loss: 2.43967, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:12:33.631424 Training: [17 epoch,  40 batch] loss: 2.44104, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:12:43.086807 Training: [17 epoch,  50 batch] loss: 2.44855, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:12:51.668142 Training: [17 epoch,  60 batch] loss: 2.49714, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:13:00.336729 Training: [17 epoch,  70 batch] loss: 2.47586, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:13:08.929544 Training: [17 epoch,  80 batch] loss: 2.44002, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:13:17.553070 Training: [17 epoch,  90 batch] loss: 2.49060, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:13:26.136760 Training: [17 epoch, 100 batch] loss: 2.46079, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:13:34.783027 Training: [17 epoch, 110 batch] loss: 2.54139, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:13:43.408470 Training: [17 epoch, 120 batch] loss: 2.46653, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:13:52.061299 Training: [17 epoch, 130 batch] loss: 2.46200, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:14:00.700279 Training: [17 epoch, 140 batch] loss: 2.52527, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:14:09.449127 Training: [17 epoch, 150 batch] loss: 2.46517, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:14:18.154746 Training: [17 epoch, 160 batch] loss: 2.45254, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:14:26.920221 Training: [17 epoch, 170 batch] loss: 2.44457, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:14:35.590637 Training: [17 epoch, 180 batch] loss: 2.47487, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:14:44.245826 Training: [17 epoch, 190 batch] loss: 2.44183, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:14:53.176758 Training: [17 epoch, 200 batch] loss: 2.44613, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:15:02.422871 Training: [17 epoch, 210 batch] loss: 2.44149, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:15:11.057856 Training: [17 epoch, 220 batch] loss: 2.42502, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:15:19.716312 Training: [17 epoch, 230 batch] loss: 2.49113, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:15:28.375863 Training: [17 epoch, 240 batch] loss: 2.46078, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:15:37.052698 Training: [17 epoch, 250 batch] loss: 2.45013, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:15:45.756140 Training: [17 epoch, 260 batch] loss: 2.45483, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:15:54.445422 Training: [17 epoch, 270 batch] loss: 2.44884, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:16:03.102058 Training: [17 epoch, 280 batch] loss: 2.48362, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:16:11.789642 Training: [17 epoch, 290 batch] loss: 2.44852, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:16:20.412359 Training: [17 epoch, 300 batch] loss: 2.46110, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:16:29.095317 Training: [17 epoch, 310 batch] loss: 2.47000, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:16:37.755908 Training: [17 epoch, 320 batch] loss: 2.49114, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:16:46.412225 Training: [17 epoch, 330 batch] loss: 2.45659, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:16:55.030779 Training: [17 epoch, 340 batch] loss: 2.44374, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:17:03.732209 Training: [17 epoch, 350 batch] loss: 2.42824, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:17:13.328752 Training: [17 epoch, 360 batch] loss: 2.49413, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:17:22.005977 Training: [17 epoch, 370 batch] loss: 2.48691, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:17:30.666214 Training: [17 epoch, 380 batch] loss: 2.47353, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "<Test> RMSE: 0.41943, MAE: 0.29177 \n",
            "2022-08-03 17:18:17.379561 Training: [18 epoch,  10 batch] loss: 2.50396, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:18:25.988838 Training: [18 epoch,  20 batch] loss: 2.54084, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:18:34.628627 Training: [18 epoch,  30 batch] loss: 2.50374, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:18:43.255692 Training: [18 epoch,  40 batch] loss: 2.44105, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:18:51.901818 Training: [18 epoch,  50 batch] loss: 2.48560, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:19:00.580782 Training: [18 epoch,  60 batch] loss: 2.48152, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:19:09.217745 Training: [18 epoch,  70 batch] loss: 2.45799, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:19:17.878019 Training: [18 epoch,  80 batch] loss: 2.43352, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:19:27.423937 Training: [18 epoch,  90 batch] loss: 2.46961, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:19:36.100275 Training: [18 epoch, 100 batch] loss: 2.44045, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:19:44.808870 Training: [18 epoch, 110 batch] loss: 2.49615, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:19:53.518334 Training: [18 epoch, 120 batch] loss: 2.45047, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:20:02.147202 Training: [18 epoch, 130 batch] loss: 2.40772, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:20:10.806989 Training: [18 epoch, 140 batch] loss: 2.43797, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:20:19.467146 Training: [18 epoch, 150 batch] loss: 2.46849, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:20:28.128016 Training: [18 epoch, 160 batch] loss: 2.47229, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:20:36.756124 Training: [18 epoch, 170 batch] loss: 2.44034, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:20:45.480718 Training: [18 epoch, 180 batch] loss: 2.47342, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:20:54.160734 Training: [18 epoch, 190 batch] loss: 2.43797, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:21:02.807363 Training: [18 epoch, 200 batch] loss: 2.46707, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:21:11.482104 Training: [18 epoch, 210 batch] loss: 2.45044, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:21:20.134629 Training: [18 epoch, 220 batch] loss: 2.58704, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:21:28.820308 Training: [18 epoch, 230 batch] loss: 2.48401, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:21:38.400750 Training: [18 epoch, 240 batch] loss: 2.44782, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:21:47.045337 Training: [18 epoch, 250 batch] loss: 2.47770, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:21:55.738558 Training: [18 epoch, 260 batch] loss: 2.46084, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:22:04.412115 Training: [18 epoch, 270 batch] loss: 2.42956, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:22:13.113171 Training: [18 epoch, 280 batch] loss: 2.43813, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:22:21.778546 Training: [18 epoch, 290 batch] loss: 2.42311, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:22:30.506727 Training: [18 epoch, 300 batch] loss: 2.47408, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:22:39.254374 Training: [18 epoch, 310 batch] loss: 2.64358, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:22:47.915309 Training: [18 epoch, 320 batch] loss: 2.46572, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:22:56.569584 Training: [18 epoch, 330 batch] loss: 2.49571, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:23:05.249509 Training: [18 epoch, 340 batch] loss: 2.47092, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:23:13.900451 Training: [18 epoch, 350 batch] loss: 2.44209, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:23:22.582705 Training: [18 epoch, 360 batch] loss: 2.44120, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:23:31.302754 Training: [18 epoch, 370 batch] loss: 2.43103, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:23:39.987943 Training: [18 epoch, 380 batch] loss: 2.45911, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "<Test> RMSE: 0.43372, MAE: 0.32173 \n",
            "2022-08-03 17:24:27.577190 Training: [19 epoch,  10 batch] loss: 2.42348, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:24:36.255825 Training: [19 epoch,  20 batch] loss: 2.56565, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:24:44.860052 Training: [19 epoch,  30 batch] loss: 2.45342, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:24:53.472198 Training: [19 epoch,  40 batch] loss: 2.45446, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:25:02.142955 Training: [19 epoch,  50 batch] loss: 2.44216, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:25:10.755581 Training: [19 epoch,  60 batch] loss: 2.43223, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:25:19.442971 Training: [19 epoch,  70 batch] loss: 2.46424, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:25:28.097014 Training: [19 epoch,  80 batch] loss: 2.48765, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:25:36.776838 Training: [19 epoch,  90 batch] loss: 2.42309, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:25:45.458747 Training: [19 epoch, 100 batch] loss: 2.44755, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:25:54.123141 Training: [19 epoch, 110 batch] loss: 2.47056, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:26:03.657519 Training: [19 epoch, 120 batch] loss: 2.49519, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:26:12.310864 Training: [19 epoch, 130 batch] loss: 2.45054, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:26:21.041238 Training: [19 epoch, 140 batch] loss: 2.46802, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:26:29.764752 Training: [19 epoch, 150 batch] loss: 2.47053, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:26:38.404057 Training: [19 epoch, 160 batch] loss: 2.45361, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:26:47.106082 Training: [19 epoch, 170 batch] loss: 2.46015, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:26:55.804048 Training: [19 epoch, 180 batch] loss: 2.44135, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:27:04.504805 Training: [19 epoch, 190 batch] loss: 2.55323, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:27:13.185233 Training: [19 epoch, 200 batch] loss: 2.41625, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:27:21.890513 Training: [19 epoch, 210 batch] loss: 2.59544, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:27:30.636616 Training: [19 epoch, 220 batch] loss: 2.45600, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:27:39.366393 Training: [19 epoch, 230 batch] loss: 2.49298, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:27:48.049749 Training: [19 epoch, 240 batch] loss: 2.43081, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:27:56.749658 Training: [19 epoch, 250 batch] loss: 2.52482, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:28:05.409908 Training: [19 epoch, 260 batch] loss: 2.46668, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:28:14.920474 Training: [19 epoch, 270 batch] loss: 2.44635, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:28:23.575709 Training: [19 epoch, 280 batch] loss: 2.46724, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:28:32.269920 Training: [19 epoch, 290 batch] loss: 2.46486, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:28:40.959395 Training: [19 epoch, 300 batch] loss: 2.47444, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:28:49.700727 Training: [19 epoch, 310 batch] loss: 2.41417, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:28:58.361649 Training: [19 epoch, 320 batch] loss: 2.46757, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:29:07.075867 Training: [19 epoch, 330 batch] loss: 2.43396, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:29:15.803555 Training: [19 epoch, 340 batch] loss: 2.46506, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:29:24.497123 Training: [19 epoch, 350 batch] loss: 2.43639, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:29:33.192715 Training: [19 epoch, 360 batch] loss: 2.48966, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:29:41.979646 Training: [19 epoch, 370 batch] loss: 2.49382, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:29:50.749334 Training: [19 epoch, 380 batch] loss: 2.43369, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "<Test> RMSE: 0.42449, MAE: 0.30294 \n",
            "2022-08-03 17:30:38.416370 Training: [20 epoch,  10 batch] loss: 2.44888, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:30:47.070885 Training: [20 epoch,  20 batch] loss: 2.44241, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:30:55.679918 Training: [20 epoch,  30 batch] loss: 2.45797, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:31:04.329814 Training: [20 epoch,  40 batch] loss: 2.42491, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:31:12.920070 Training: [20 epoch,  50 batch] loss: 2.44454, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:31:21.511617 Training: [20 epoch,  60 batch] loss: 2.44858, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:31:30.102324 Training: [20 epoch,  70 batch] loss: 2.50455, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:31:38.748688 Training: [20 epoch,  80 batch] loss: 2.41868, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:31:47.481421 Training: [20 epoch,  90 batch] loss: 2.46203, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:31:56.198086 Training: [20 epoch, 100 batch] loss: 2.43308, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:32:04.824824 Training: [20 epoch, 110 batch] loss: 2.43697, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:32:13.484214 Training: [20 epoch, 120 batch] loss: 2.43543, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:32:22.150309 Training: [20 epoch, 130 batch] loss: 2.43733, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:32:31.610175 Training: [20 epoch, 140 batch] loss: 2.47658, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:32:40.395094 Training: [20 epoch, 150 batch] loss: 2.52623, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:32:49.110955 Training: [20 epoch, 160 batch] loss: 2.45192, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:32:57.802020 Training: [20 epoch, 170 batch] loss: 2.43244, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:33:06.506322 Training: [20 epoch, 180 batch] loss: 2.43663, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:33:15.191177 Training: [20 epoch, 190 batch] loss: 2.45508, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:33:23.905130 Training: [20 epoch, 200 batch] loss: 2.66181, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:33:32.588018 Training: [20 epoch, 210 batch] loss: 2.44205, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:33:41.313163 Training: [20 epoch, 220 batch] loss: 2.51286, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:33:49.995028 Training: [20 epoch, 230 batch] loss: 2.51956, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:33:58.701597 Training: [20 epoch, 240 batch] loss: 2.47191, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:34:07.365885 Training: [20 epoch, 250 batch] loss: 2.44594, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:34:16.032499 Training: [20 epoch, 260 batch] loss: 2.46432, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:34:24.717395 Training: [20 epoch, 270 batch] loss: 2.46863, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:34:33.433347 Training: [20 epoch, 280 batch] loss: 2.41871, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:34:42.998472 Training: [20 epoch, 290 batch] loss: 2.45516, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:34:51.747368 Training: [20 epoch, 300 batch] loss: 2.42508, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:35:00.458552 Training: [20 epoch, 310 batch] loss: 2.60555, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:35:09.107932 Training: [20 epoch, 320 batch] loss: 2.47694, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:35:17.795983 Training: [20 epoch, 330 batch] loss: 2.47742, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:35:26.525951 Training: [20 epoch, 340 batch] loss: 2.46491, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:35:35.092749 Training: [20 epoch, 350 batch] loss: 2.43977, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:35:43.790851 Training: [20 epoch, 360 batch] loss: 2.41173, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:35:52.520922 Training: [20 epoch, 370 batch] loss: 2.43531, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:36:01.213941 Training: [20 epoch, 380 batch] loss: 2.52230, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "<Test> RMSE: 0.42989, MAE: 0.31418 \n",
            "2022-08-03 17:36:48.116435 Training: [21 epoch,  10 batch] loss: 2.43190, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:36:57.536634 Training: [21 epoch,  20 batch] loss: 2.45240, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:37:06.109164 Training: [21 epoch,  30 batch] loss: 2.44835, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:37:14.744158 Training: [21 epoch,  40 batch] loss: 2.44451, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:37:23.345001 Training: [21 epoch,  50 batch] loss: 2.44236, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:37:31.931238 Training: [21 epoch,  60 batch] loss: 2.44909, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:37:40.583269 Training: [21 epoch,  70 batch] loss: 2.51287, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:37:49.235339 Training: [21 epoch,  80 batch] loss: 2.48899, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:37:57.974361 Training: [21 epoch,  90 batch] loss: 2.45523, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:38:06.588900 Training: [21 epoch, 100 batch] loss: 2.46923, the best RMSE/MAE: 0.38818 / 0.16549\n",
            "2022-08-03 17:38:15.249434 Training: [21 epoch, 110 batch] loss: 2.46713, the best RMSE/MAE: 0.38818 / 0.16549\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "a40NNTZVBd-M"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}